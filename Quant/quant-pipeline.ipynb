{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install gptqmodel\n",
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Imports and Setup\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AwqConfig, GPTQConfig\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "import subprocess\n",
    "\n",
    "# Enable CUDA debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "# Set your Hugging Face token\n",
    "HF_TOKEN = \"\"\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "OUTPUT_DIR = \"/kaggle/working/quantized_model\"\n",
    "REPO_ID = \"msaadg/Llama-3.2-3B-quantized\"\n",
    "DEVICE_MAP = \"auto\"\n",
    "TORCH_DTYPE = torch.bfloat16  # Use bfloat16 for non-quantized weights where supported\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper function for calibration dataset\n",
    "def get_calibration_dataset(dataset_name=\"allenai/c4\", split=\"train\", num_samples=100):\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_name, \"en\", split=split, streaming=True, trust_remote_code=True)\n",
    "        samples = [next(iter(dataset))[\"text\"] for _ in range(num_samples)]\n",
    "        return samples\n",
    "    except Exception as e:\n",
    "        print(f\"Error in dataset preparation: {e}\")\n",
    "        raise\n",
    "\n",
    "# Cell 5: GPTQ Quantization (4-bit)\n",
    "def quantize_gptq():\n",
    "    print(\"Quantizing with GPTQ (4-bit)...\")\n",
    "\n",
    "    # Load calibration dataset subset\n",
    "    dataset = get_calibration_dataset(dataset_name=\"allenai/c4\", num_samples=100)\n",
    "\n",
    "    quantization_config = GPTQConfig(\n",
    "        bits=4,\n",
    "        dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=DEVICE_MAP,\n",
    "            torch_dtype=TORCH_DTYPE\n",
    "        )\n",
    "        # Save locally\n",
    "        model.save_pretrained(os.path.join(OUTPUT_DIR, \"llama3.2-3b-gptq-4bit\"))\n",
    "        tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"llama3.2-3b-gptq-4bit\"))\n",
    "        # Push to Hugging Face Hub\n",
    "        model.push_to_hub(f\"{REPO_ID}-gptq-4bit\")\n",
    "        tokenizer.push_to_hub(f\"{REPO_ID}-gptq-4bit\")\n",
    "        print(f\"GPTQ quantized model saved to {os.path.join(OUTPUT_DIR, 'llama3.2-3b-gptq-4bit')} and pushed to {REPO_ID}-gptq-4bit\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPTQ quantization: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Main Execution\n",
    "def main():\n",
    "    if QUANT_METHOD == \"bnb\":\n",
    "        model = quantize_bnb()\n",
    "    elif QUANT_METHOD == \"gptq\":\n",
    "        model = quantize_gptq()\n",
    "    elif QUANT_METHOD == \"awq\":\n",
    "        model = quantize_awq()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid QUANT_METHOD: {QUANT_METHOD}. Choose from 'bnb', 'awq', 'gptq'.\")\n",
    "    \n",
    "    # Print memory footprint\n",
    "    # print(f\"Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_sparse_dsnot_neo2\"\n",
    "REPO_ID = \"msaadg/llama_3b_sparse_dsnot_neo2-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_sparse_neo\"\n",
    "REPO_ID = \"msaadg/llama_3b_sparse_neo-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_wanda_neo\"\n",
    "REPO_ID = \"msaadg/llama_3b_wanda_neo-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_wanda_dsnot_neo\"\n",
    "REPO_ID = \"msaadg/llama_3b_wanda_dsnot_neo-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_pruned_neo\"\n",
    "REPO_ID = \"msaadg/llama_3b_pruned_neo-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_sparse_dsnot_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_sparse_dsnot_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_sparse_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_sparse_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_wanda_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_wanda_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_wanda_dsnot_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_wanda_dsnot_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_pruned_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_pruned_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"gptq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BnB & AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install optimum\n",
    "!pip install --upgrade transformers accelerate bitsandbytes\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: BitsAndBytes Quantization (4-bit)\n",
    "def quantize_bnb():\n",
    "    print(\"Quantizing with BitsAndBytes (4-bit)...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",  # Normal Float 4 for better training compatibility\n",
    "        bnb_4bit_compute_dtype=TORCH_DTYPE,\n",
    "        bnb_4bit_use_double_quant=True  # Nested quantization for extra memory savings\n",
    "    )\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=DEVICE_MAP,\n",
    "            torch_dtype=TORCH_DTYPE\n",
    "        )\n",
    "        # Save locally\n",
    "        model.save_pretrained(os.path.join(OUTPUT_DIR, \"llama3.2-3b-bnb-4bit\"))\n",
    "        tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"llama3.2-3b-bnb-4bit\"))\n",
    "        # Push to Hugging Face Hub\n",
    "        model.push_to_hub(f\"{REPO_ID}-bnb-4bit\")\n",
    "        tokenizer.push_to_hub(f\"{REPO_ID}-bnb-4bit\")\n",
    "        print(f\"BNB quantized model saved to {os.path.join(OUTPUT_DIR, 'llama3.2-3b-bnb-4bit')} and pushed to {REPO_ID}-bnb-4bit\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in BNB quantization: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: AWQ Quantization (4-bit)\n",
    "def quantize_awq():\n",
    "    print(\"Quantizing with AWQ (4-bit)...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    from awq import AutoAWQForCausalLM\n",
    "    from datasets import load_dataset\n",
    "    from huggingface_hub import upload_folder, create_repo\n",
    "    try:\n",
    "        # Load calibration dataset\n",
    "        dataset_path = \"carlosejimenez/wikitext__wikitext-2-raw-v1\"\n",
    "        data = load_dataset(dataset_path, split=\"train\")\n",
    "        calib_data = [text for text in data[\"text\"] if text.strip() != '' and len(text.split(' ')) > 20]\n",
    "\n",
    "        # Quantization configuration for AutoAWQ\n",
    "        quant_config = {\n",
    "            \"zero_point\": True,\n",
    "            \"q_group_size\": 128,\n",
    "            \"w_bit\": 4,\n",
    "            \"version\": \"GEMM\"\n",
    "        }\n",
    "        model = AutoAWQForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            safetensors=True,\n",
    "            device_map=DEVICE_MAP,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        model.quantize(tokenizer, quant_config=quant_config, calib_data=calib_data)\n",
    "        \n",
    "        # Save locally with sharding\n",
    "        quant_path = os.path.join(OUTPUT_DIR, \"llama3.2-3b-awq-4bit\")\n",
    "        model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\n",
    "        tokenizer.save_pretrained(quant_path)\n",
    "        \n",
    "        # Create repository on Hugging Face Hub if it doesn't exist\n",
    "        create_repo(repo_id=f\"{REPO_ID}-awq-4bit\", repo_type=\"model\", token=HF_TOKEN, exist_ok=True)\n",
    "        \n",
    "        # Push to Hugging Face Hub\n",
    "        upload_folder(\n",
    "            folder_path=quant_path,\n",
    "            repo_id=f\"{REPO_ID}-awq-4bit\",\n",
    "            repo_type=\"model\",\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        print(f\"AWQ quantized model saved to {quant_path} and pushed to {REPO_ID}-awq-4bit\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in AWQ quantization: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_sparse_dsnot_neo2\"\n",
    "REPO_ID = \"msaadg/llama_3b_sparse_dsnot_neo2-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_sparse_neo\"\n",
    "REPO_ID = \"msaadg/llama_3b_sparse_neo-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_wanda_neo\"\n",
    "REPO_ID = \"msaadg/llama_3b_wanda_neo-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_wanda_dsnot_neo\"\n",
    "REPO_ID = \"msaadg/llama_3b_wanda_dsnot_neo-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_pruned_neo\"\n",
    "REPO_ID = \"msaadg/llama_3b_pruned_neo-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_sparse_dsnot_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_sparse_dsnot_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_sparse_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_sparse_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_wanda_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_wanda_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_wanda_dsnot_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_wanda_dsnot_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and configuration\n",
    "MODEL_ID = \"musab1blaser/llama_3b_pruned_mini\"\n",
    "REPO_ID = \"msaadg/llama_3b_pruned_mini-quantized\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "QUANT_METHOD = \"bnb\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()\n",
    "\n",
    "QUANT_METHOD = \"awq\"  # Options: \"bnb\", \"gptq\", \"awq\"\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
