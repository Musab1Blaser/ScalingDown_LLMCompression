{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:51:19.850841Z",
     "iopub.status.busy": "2025-07-31T02:51:19.850437Z",
     "iopub.status.idle": "2025-07-31T02:52:56.698792Z",
     "shell.execute_reply": "2025-07-31T02:52:56.697919Z",
     "shell.execute_reply.started": "2025-07-31T02:51:19.850813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.47.0\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==1.1.1\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.1.1) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.1.1) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.0) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.0) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.0) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.47.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.47.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.47.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.47.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.47.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.47.0) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate==1.1.1)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate==1.1.1)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate==1.1.1)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate==1.1.1)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate==1.1.1)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate==1.1.1)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate==1.1.1)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.1.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==1.1.1) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==1.1.1) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.47.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.47.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.47.0) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.47.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.47.0) (2024.2.0)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, accelerate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.5.2\n",
      "    Uninstalling accelerate-1.5.2:\n",
      "      Successfully uninstalled accelerate-1.5.2\n",
      "Successfully installed accelerate-1.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.47.0 accelerate==1.1.1   ### VERY IMPORTANT, TOOK ME A LONG WHILE TO UNDERSTAND THAT IT WORKS ON THIS VERSION (on Kaggle)\n",
    "#  torch==2.5.1+cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:52:56.700829Z",
     "iopub.status.busy": "2025-07-31T02:52:56.700554Z",
     "iopub.status.idle": "2025-07-31T02:52:56.941397Z",
     "shell.execute_reply": "2025-07-31T02:52:56.940613Z",
     "shell.execute_reply.started": "2025-07-31T02:52:56.700804Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 31 02:52:56 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   39C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:12.180241Z",
     "iopub.status.busy": "2025-07-30T02:47:12.180005Z",
     "iopub.status.idle": "2025-07-30T02:47:13.265980Z",
     "shell.execute_reply": "2025-07-30T02:47:13.265317Z",
     "shell.execute_reply.started": "2025-07-30T02:47:12.180219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DSnoT'...\n",
      "remote: Enumerating objects: 283, done.\u001b[K\n",
      "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
      "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
      "remote: Total 283 (delta 91), reused 143 (delta 51), pack-reused 94 (from 1)\u001b[K\n",
      "Receiving objects: 100% (283/283), 1.55 MiB | 10.90 MiB/s, done.\n",
      "Resolving deltas: 100% (130/130), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AlphaAnas/DSnoT.git # this repository on Github contains the implementation / fork of DSnoT finetuning method\n",
    "                                                # which contains DSnoT code and SparseGPT, and Wanda pruning methods. I also added normal L1 Pruning method \n",
    "                                                # there copying from Torch Pruning repository\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:13.267427Z",
     "iopub.status.busy": "2025-07-30T02:47:13.267017Z",
     "iopub.status.idle": "2025-07-30T02:47:13.272451Z",
     "shell.execute_reply": "2025-07-30T02:47:13.271805Z",
     "shell.execute_reply.started": "2025-07-30T02:47:13.267398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/DSnoT\n"
     ]
    }
   ],
   "source": [
    "%cd DSnoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:13.274835Z",
     "iopub.status.busy": "2025-07-30T02:47:13.274672Z",
     "iopub.status.idle": "2025-07-30T02:47:13.405548Z",
     "shell.execute_reply": "2025-07-30T02:47:13.404748Z",
     "shell.execute_reply.started": "2025-07-30T02:47:13.274822Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t\t  .gitignore\t\t\t  main.py\n",
      "..\t\t  imgs\t\t\t\t  pruning-dsnot-procedure.ipynb\n",
      "DEBUG_GUIDE.md\t  lib\t\t\t\t  __pycache__\n",
      "debug_run.ps1\t  llama-3b-DSnoT-results.txt\t  README.md\n",
      "debug_run.py\t  llama-3b-l1-results.txt\t  torch_pruning\n",
      "environment.yaml  llama-3b-sparsegpt-results.txt\n",
      ".git\t\t  magnitude.py\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:13.406761Z",
     "iopub.status.busy": "2025-07-30T02:47:13.406519Z",
     "iopub.status.idle": "2025-07-30T02:47:13.983217Z",
     "shell.execute_reply": "2025-07-30T02:47:13.982553Z",
     "shell.execute_reply.started": "2025-07-30T02:47:13.406739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNLOAD THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:13.984328Z",
     "iopub.status.busy": "2025-07-30T02:47:13.984135Z",
     "iopub.status.idle": "2025-07-30T02:47:16.406199Z",
     "shell.execute_reply": "2025-07-30T02:47:16.405382Z",
     "shell.execute_reply.started": "2025-07-30T02:47:13.984306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Download the dataset files\n",
    "!wget -q https://huggingface.co/datasets/allenai/c4/resolve/main/en/c4-train.00000-of-01024.json.gz\n",
    "!wget -q https://huggingface.co/datasets/allenai/c4/resolve/main/en/c4-validation.00000-of-00008.json.gz\n",
    "\n",
    "# Step 2: Create the 'en' directory (if it doesn't exist)\n",
    "!mkdir -p en\n",
    "\n",
    "# Step 3: Move the downloaded files into the 'en' directory\n",
    "!mv c4-train.00000-of-01024.json.gz en/\n",
    "!mv c4-validation.00000-of-00008.json.gz en/\n",
    "\n",
    "# Optional Step 4: Change directory to working directory (not needed unless your code explicitly requires it)\n",
    "# %cd /kaggle/working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Loading the dataset manually to avoid errors in the pre-written code of library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:16.409425Z",
     "iopub.status.busy": "2025-07-30T02:47:16.409206Z",
     "iopub.status.idle": "2025-07-30T02:47:27.635212Z",
     "shell.execute_reply": "2025-07-30T02:47:27.634648Z",
     "shell.execute_reply.started": "2025-07-30T02:47:16.409388Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76d5a3ae99f414fa74bb41bd9590751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b40bf3fb87b4bf198b50d803246e0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "traindata = load_dataset('json', data_files='en/c4-train.00000-of-01024.json.gz', split='train')\n",
    "valdata = load_dataset('json', data_files='en/c4-validation.00000-of-00008.json.gz', split='train')  # still 'train' because there's only one split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default arguments - each method will edit this below as per its requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:27.636362Z",
     "iopub.status.busy": "2025-07-30T02:47:27.635980Z",
     "iopub.status.idle": "2025-07-30T02:47:27.641257Z",
     "shell.execute_reply": "2025-07-30T02:47:27.640636Z",
     "shell.execute_reply.started": "2025-07-30T02:47:27.636344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    model_type = \"llama\"  # will be inferred automatically\n",
    "    seed = 0\n",
    "    nsamples = 128\n",
    "    eval_dataset = 'wikitext2'\n",
    "    sparsity_ratio = 0.2\n",
    "    sparsity_type = 'unstructured'\n",
    "    prune_method = 'magnitude'\n",
    "    initial_method = 'magnitude'\n",
    "    max_cycle_time = 50\n",
    "    without_DSnoT = True\n",
    "    update_threshold = 0.1\n",
    "    pow_of_var_regrowing = 1\n",
    "    pow_of_var_pruning = 1  # default not overridden\n",
    "    skip_layer = 'no_skip'\n",
    "    skip_sub_layer = 'no_skip'\n",
    "    without_same_sign = 'True'\n",
    "    get_time_overhead = False\n",
    "    output_results_file = 'results.txt'\n",
    "    cache_dir = 'llama-3b-weights'\n",
    "    save_model = \"pruned-llama-3b-L2\"\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function of loading the model and calling the Pruning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:27.642353Z",
     "iopub.status.busy": "2025-07-30T02:47:27.642099Z",
     "iopub.status.idle": "2025-07-30T02:47:44.956758Z",
     "shell.execute_reply": "2025-07-30T02:47:44.956082Z",
     "shell.execute_reply.started": "2025-07-30T02:47:27.642329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.6.0+cu124\n",
      "transformers 4.47.0\n",
      "accelerate 1.1.1\n",
      "# of gpus:  2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from importlib.metadata import version\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "from lib.prune import check_sparsity, prune_DSnoT, prune_magnitude, prune_sparsegpt, prune_wanda\n",
    "from lib.prune_opt import check_sparsity_opt, prune_DSnoT_opt\n",
    "from lib.eval import eval_ppl\n",
    "from lib.save_results import save_ppl_result\n",
    "from lib.data import get_loaders, TokenizerWrapper # Import TokenizerWrapper\n",
    "\n",
    "\n",
    "\n",
    "from magnitude import prune_model\n",
    "\n",
    "print('torch', version('torch'))\n",
    "print('transformers', version('transformers'))\n",
    "print('accelerate', version('accelerate'))\n",
    "print('# of gpus: ', torch.cuda.device_count())\n",
    "\n",
    "\n",
    "\n",
    "def get_llm(model, cache_dir=\"llm_weights\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model,\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=cache_dir,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    model.seqlen = 2048\n",
    "    # model.seqlen = 128\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "\n",
    "        # Set random seeds\n",
    "        np.random.seed(args.seed)\n",
    "        torch.random.manual_seed(args.seed)\n",
    "\n",
    "        # Determine model type\n",
    "        if not args.model_type:\n",
    "            if any(model_name in args.model for model_name in [\"llama\", \"vicuna\"]):\n",
    "                args.model_type = \"llama\"\n",
    "            elif \"opt\" in args.model:\n",
    "                args.model_type = \"opt\"\n",
    "            else:\n",
    "                print(\"Warning: Could not determine model type from model name.\")\n",
    "                return\n",
    "        print(f\"model type: {args.model_type}\")\n",
    "\n",
    "        prune_n, prune_m = 0, 0\n",
    "        if args.sparsity_type != \"unstructured\":\n",
    "            assert args.sparsity_ratio == 0.5, \"sparsity ratio must be 0.5 for structured N:M sparsity\"\n",
    "            prune_n, prune_m = map(int, args.sparsity_type.split(\":\"))\n",
    "\n",
    "        print(f\"loading llm model {args.model}\")\n",
    "        model = get_llm(args.model, args.cache_dir)\n",
    "        model.eval()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "            # Check initial sparsity\n",
    "        initial_sparsity = check_sparsity(model)\n",
    "        print(f\"Initial sparsity: {initial_sparsity:.4f}\")\n",
    "        \n",
    "        # Get initial parameter count\n",
    "        initial_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "        print(f\"Initial parameters: {initial_params:.2f}M\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "        if \"30b\" in args.model or \"65b\" in args.model:\n",
    "            device = model.hf_device_map[\"lm_head\"]\n",
    "\n",
    "        \n",
    "        print(f\"Using device: {device}\")\n",
    "        print(f\"Model: {args.model}\")\n",
    "        print(f\"Sparsity ratio: {args.sparsity_ratio}\")\n",
    "        print(f\"Samples: {args.nsamples}\")\n",
    "\n",
    "        if args.sparsity_ratio != 0:\n",
    "            print(\"pruning starts\")\n",
    "\n",
    "\n",
    "            # we are currently only dealing with llama\n",
    "            if args.model_type == \"llama\":\n",
    "                if args.prune_method == \"wanda\":\n",
    "                    prune_wanda(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "\n",
    "\n",
    "                # normal l1 pruning\n",
    "                elif args.prune_method == \"magnitude_normal\":\n",
    "\n",
    "                    prune_magnitude(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "\n",
    "                # https://github.com/VainF/Torch-Pruning - magnitude pruning using dep-graph\n",
    "                elif args.prune_method == \"magnitude_torch_prune\":\n",
    "\n",
    "\n",
    "                    prune_model(\n",
    "                        model, tokenizer,\n",
    "                        pruning_type=args.prune_method,\n",
    "                        weight_metric=\"l1\", model_type=args.model_type,\n",
    "                        device=device,\n",
    "                        max_seq_len=2048, pruning_ratio=args.sparsity_ratio,\n",
    "                        save_path=args.save_path\n",
    "                            )\n",
    "                    \n",
    "                elif args.prune_method == \"sparsegpt\":\n",
    "                          prune_sparsegpt(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m, save_path=args.save_path)\n",
    "\n",
    "                # a fine tuning method that would prune using anyof the above method (currently done using wanda)\n",
    "                elif args.prune_method == \"DSnoT\":\n",
    "                    prune_DSnoT(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m, save_path=args.save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # I HAVEN'T USED OR IMPLEMENTED THE OPT based MODELS \n",
    "        # elif args.model_type == \"opt\":\n",
    "        #     if args.prune_method == \"wanda\":\n",
    "        #         prune_wanda_opt(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "        #     elif args.prune_method == \"magnitude\":\n",
    "        #         prune_magnitude_opt(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "        #     elif args.prune_method == \"sparsegpt\":\n",
    "        #         prune_sparsegpt_opt(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "        #     elif args.prune_method == \"DSnoT\":\n",
    "        #         prune_DSnoT_opt(args, model, tokenizer, device, prune_n=0, prune_m=0, save_path=args.save_path)\n",
    "\n",
    "        print(\"*\" * 30)\n",
    "        sparsity_ratio = check_sparsity(model) if args.model_type == \"llama\" else check_sparsity_opt(model)\n",
    "        print(f\"sparsity sanity check {sparsity_ratio:.4f}\")\n",
    "        print(\"*\" * 30)\n",
    "\n",
    "    # Get final parameter count\n",
    "        final_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "        print(f\"Final parameters: {final_params:.2f}M\")\n",
    "\n",
    "        compression_ratio = (initial_params - final_params) / initial_params * 100\n",
    "        print(f\"Compression ratio: {compression_ratio:.2f}%\")\n",
    "        \n",
    "\n",
    "        # dataset = 'wikitext2'\n",
    "        # ppl = eval_ppl(model, tokenizer, dataset, device)\n",
    "        # print(f\"\\nppl on {dataset}: {ppl}\\n\")\n",
    "\n",
    "        # save_ppl_result(args, args.output_results_file, sparsity_ratio, ppl)\n",
    "\n",
    "        if args.save_model:\n",
    "            model.save_pretrained(args.save_model)\n",
    "            tokenizer.save_pretrained(args.save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Perplxity before pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T20:46:02.185810Z",
     "iopub.status.busy": "2025-07-29T20:46:02.185547Z",
     "iopub.status.idle": "2025-07-29T20:46:02.189691Z",
     "shell.execute_reply": "2025-07-29T20:46:02.188970Z",
     "shell.execute_reply.started": "2025-07-29T20:46:02.185787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating perplexity Before Pruning\")\n",
    "model = get_llm(args.model, args.cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)\n",
    "dataset = 'wikitext2'\n",
    "device = torch.device(\"cuda:0\")\n",
    "ppl = eval_ppl(model, tokenizer, dataset, device)\n",
    "print(f\"\\nppl on {dataset}: {ppl}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING Torch Pruning , Dep-Graph L1 PRUNING ON LLAMA 3B (working)\n",
    "\n",
    " Note: 1- (1-pruning_ratio) ^2 will be the real pruning ratio \n",
    "so if the pruning ratio = 0.3 then the model will be 0.5 % pruned (only for this dep-graph version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:44.957989Z",
     "iopub.status.busy": "2025-07-30T02:47:44.957491Z",
     "iopub.status.idle": "2025-07-30T02:47:44.962281Z",
     "shell.execute_reply": "2025-07-30T02:47:44.961510Z",
     "shell.execute_reply.started": "2025-07-30T02:47:44.957963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "args.model_type = \"llama\"  # will be inferred automatically\n",
    "args.sparsity_ratio = 0.17 # setting 0.1 so pruning ratio becomes roughly 0.19 \n",
    "args.sparsity_type = 'unstructured'\n",
    "args.prune_method = 'magnitude_torch_prune' # right now it only supports L1 pruning - will add L2 as well\n",
    "args.initial_method = 'magnitude_torch_prune'\n",
    "args.output_results_file = 'llama-3b-l2-torch_prune-results.txt'\n",
    "# args.cache_dir = r'C:\\Users\\hp-15\\Disc D\\scrapeyard\\GSCP\\pruning\\DSNOT2\\babylm-10m-weights'\n",
    "# args.cache_dir = '/home/meesum/.cache/huggingface/hub'\n",
    "# args.save_model = \"pruned-llama-3b_torch_prune-l2\"\n",
    "args.cache_dir = 'llama-3b-weights'\n",
    "args.save_path = \"pruned-llama-3b_torch_prune-l1_20_per\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:47:44.963486Z",
     "iopub.status.busy": "2025-07-30T02:47:44.963086Z",
     "iopub.status.idle": "2025-07-30T02:50:01.635543Z",
     "shell.execute_reply": "2025-07-30T02:50:01.634913Z",
     "shell.execute_reply.started": "2025-07-30T02:47:44.963462Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: llama\n",
      "loading llm model meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536f9819884546349118a69943d5c999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 02:47:47.647950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753843667.989848      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753843668.097022      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ed928173e49aea318b57e3d90d2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad3f4588a1749b5addc0869dc2d4f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aedc472899c4003bff516bda187114b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9f1472fe544e80a4989565ddb08ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8190db88d14212b08ab0fea908895a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9caea0842f4978867cd87be7e5a84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0290802444664f9ca0a036082919ff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1485a5da2d9e42adbc6bacef892a50c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33079f199a745ca9867ecdeb8297648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 sparsity 0.000002\n",
      "layer 1 sparsity 0.000001\n",
      "layer 2 sparsity 0.000001\n",
      "layer 3 sparsity 0.000001\n",
      "layer 4 sparsity 0.000001\n",
      "layer 5 sparsity 0.000002\n",
      "layer 6 sparsity 0.000001\n",
      "layer 7 sparsity 0.000001\n",
      "layer 8 sparsity 0.000001\n",
      "layer 9 sparsity 0.000001\n",
      "layer 10 sparsity 0.000001\n",
      "layer 11 sparsity 0.000002\n",
      "layer 12 sparsity 0.000002\n",
      "layer 13 sparsity 0.000002\n",
      "layer 14 sparsity 0.000002\n",
      "layer 15 sparsity 0.000001\n",
      "layer 16 sparsity 0.000001\n",
      "layer 17 sparsity 0.000001\n",
      "layer 18 sparsity 0.000001\n",
      "layer 19 sparsity 0.000001\n",
      "layer 20 sparsity 0.000001\n",
      "layer 21 sparsity 0.000001\n",
      "layer 22 sparsity 0.000001\n",
      "layer 23 sparsity 0.000001\n",
      "layer 24 sparsity 0.000001\n",
      "layer 25 sparsity 0.000001\n",
      "layer 26 sparsity 0.000001\n",
      "layer 27 sparsity 0.000001\n",
      "Initial sparsity: 0.0000\n",
      "Initial parameters: 3212.75M\n",
      "Using device: cuda:0\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Sparsity ratio: 0.17\n",
      "Samples: 128\n",
      "pruning starts\n",
      "Original model parameters: 3212.75M\n",
      "Warning: Unknown pruning type 'magnitude_torch_prune'. Using magnitude l2 importance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/DSnoT/torch_pruning/dependency.py:712: UserWarning: Unwrapped parameters detected: ['model.layers.19.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.norm.weight'].\n",
      " Torch-Pruning will prune the last non-singleton dimension of these parameters. If you wish to change this behavior, please provide an unwrapped_parameters argument.\n",
      "  warnings.warn(warning_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pruning process...\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072) => (embed_tokens): Embedding(128256, 2548)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False) => (q_proj): Linear(in_features=2548, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False) => (k_proj): Linear(in_features=2548, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False) => (v_proj): Linear(in_features=2548, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False) => (o_proj): Linear(in_features=2048, out_features=2548, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False) => (gate_proj): Linear(in_features=2548, out_features=6796, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False) => (up_proj): Linear(in_features=2548, out_features=6796, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False) => (down_proj): Linear(in_features=6796, out_features=2548, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05) => (input_layernorm): LlamaRMSNorm((2548,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05) => (post_attention_layernorm): LlamaRMSNorm((2548,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05) => (norm): LlamaRMSNorm((2548,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False) => (lm_head): Linear(in_features=2548, out_features=128256, bias=False)\n",
      ")\n",
      "\n",
      "Validating parameter count after full pruning and reconfiguration...\n",
      "✅ Final Pruned Model Parameters: 2546.64M\n",
      "Compression ratio: 20.73%\n",
      "Saving pruned model to pruned-llama-3b_torch_prune-l1_20_per...\n",
      "Pruned model saved successfully to pruned-llama-3b_torch_prune-l1_20_per\n",
      "******************************\n",
      "layer 0 sparsity 0.000001\n",
      "layer 1 sparsity 0.000001\n",
      "layer 2 sparsity 0.000001\n",
      "layer 3 sparsity 0.000001\n",
      "layer 4 sparsity 0.000001\n",
      "layer 5 sparsity 0.000002\n",
      "layer 6 sparsity 0.000001\n",
      "layer 7 sparsity 0.000001\n",
      "layer 8 sparsity 0.000001\n",
      "layer 9 sparsity 0.000001\n",
      "layer 10 sparsity 0.000001\n",
      "layer 11 sparsity 0.000001\n",
      "layer 12 sparsity 0.000002\n",
      "layer 13 sparsity 0.000001\n",
      "layer 14 sparsity 0.000001\n",
      "layer 15 sparsity 0.000001\n",
      "layer 16 sparsity 0.000001\n",
      "layer 17 sparsity 0.000001\n",
      "layer 18 sparsity 0.000001\n",
      "layer 19 sparsity 0.000001\n",
      "layer 20 sparsity 0.000001\n",
      "layer 21 sparsity 0.000001\n",
      "layer 22 sparsity 0.000001\n",
      "layer 23 sparsity 0.000001\n",
      "layer 24 sparsity 0.000001\n",
      "layer 25 sparsity 0.000001\n",
      "layer 26 sparsity 0.000001\n",
      "layer 27 sparsity 0.000001\n",
      "sparsity sanity check 0.0000\n",
      "******************************\n",
      "Final parameters: 2546.64M\n",
      "Compression ratio: 20.73%\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING Normal L1/L2 PRUNING ON LLAMA 3B (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T14:33:18.100900Z",
     "iopub.status.busy": "2025-07-24T14:33:18.100607Z",
     "iopub.status.idle": "2025-07-24T14:33:18.105132Z",
     "shell.execute_reply": "2025-07-24T14:33:18.104277Z",
     "shell.execute_reply.started": "2025-07-24T14:33:18.100880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "args.model_type = \"llama\"  # will be inferred automatically\n",
    "args.sparsity_ratio = 0.2\n",
    "args.sparsity_type = 'unstructured'\n",
    "args.prune_method = 'magnitude_normal' # right now it only supports L1 pruning - will add L2 as well\n",
    "args.initial_method = 'magnitude_normal'\n",
    "args.output_results_file = 'llama-3b-l1-results.txt'\n",
    "args.cache_dir = 'llama-3b-weights'\n",
    "args.save_model = \"pruned-llama-3b-l1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-24T14:33:18.707357Z",
     "iopub.status.busy": "2025-07-24T14:33:18.706814Z",
     "iopub.status.idle": "2025-07-24T14:33:47.647555Z",
     "shell.execute_reply": "2025-07-24T14:33:47.646916Z",
     "shell.execute_reply.started": "2025-07-24T14:33:18.707332Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: llama\n",
      "loading llm model meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bbac691f6944c082df514bd287ebb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device  cuda:0\n",
      "pruning starts\n",
      "******************************\n",
      "layer 0 sparsity 0.200704\n",
      "layer 1 sparsity 0.200664\n",
      "layer 2 sparsity 0.200563\n",
      "layer 3 sparsity 0.200524\n",
      "layer 4 sparsity 0.200458\n",
      "layer 5 sparsity 0.200772\n",
      "layer 6 sparsity 0.200737\n",
      "layer 7 sparsity 0.200478\n",
      "layer 8 sparsity 0.200906\n",
      "layer 9 sparsity 0.200651\n",
      "layer 10 sparsity 0.200559\n",
      "layer 11 sparsity 0.200568\n",
      "layer 12 sparsity 0.200702\n",
      "layer 13 sparsity 0.200883\n",
      "layer 14 sparsity 0.200824\n",
      "layer 15 sparsity 0.200509\n",
      "layer 16 sparsity 0.200653\n",
      "layer 17 sparsity 0.200361\n",
      "layer 18 sparsity 0.200644\n",
      "layer 19 sparsity 0.200709\n",
      "layer 20 sparsity 0.200461\n",
      "layer 21 sparsity 0.201008\n",
      "layer 22 sparsity 0.200360\n",
      "layer 23 sparsity 0.200439\n",
      "layer 24 sparsity 0.200517\n",
      "layer 25 sparsity 0.200541\n",
      "layer 26 sparsity 0.200574\n",
      "layer 27 sparsity 0.200991\n",
      "sparsity sanity check 0.2006\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T17:43:39.582498Z",
     "iopub.status.busy": "2025-07-26T17:43:39.582249Z",
     "iopub.status.idle": "2025-07-26T17:43:39.586708Z",
     "shell.execute_reply": "2025-07-26T17:43:39.586003Z",
     "shell.execute_reply.started": "2025-07-26T17:43:39.582480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory of GPU: 0.00 MB\n",
      "Cached Memory of GPU: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory of GPU: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory of GPU: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T08:03:21.983038Z",
     "iopub.status.busy": "2025-07-26T08:03:21.982786Z",
     "iopub.status.idle": "2025-07-26T08:03:22.174331Z",
     "shell.execute_reply": "2025-07-26T08:03:22.173662Z",
     "shell.execute_reply.started": "2025-07-26T08:03:21.983022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete all relevant variables\n",
    "# del model  # Replace with your model variable name\n",
    "# del inputs, outputs  # Replace with any other tensors or variables\n",
    "gc.collect()  # Run Python garbage collector\n",
    "torch.cuda.empty_cache()  # Clear PyTorch's GPU cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T16:34:58.739358Z",
     "iopub.status.busy": "2025-07-26T16:34:58.738583Z",
     "iopub.status.idle": "2025-07-26T16:34:58.744076Z",
     "shell.execute_reply": "2025-07-26T16:34:58.743386Z",
     "shell.execute_reply.started": "2025-07-26T16:34:58.739332Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory of GPU: 0.00 MB\n",
      "Cached Memory of GPU: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory of GPU: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory of GPU: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING DSnoT PRUNING ON LLAMA 3B (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:41:29.358208Z",
     "iopub.status.busy": "2025-07-26T18:41:29.357883Z",
     "iopub.status.idle": "2025-07-26T18:41:29.363933Z",
     "shell.execute_reply": "2025-07-26T18:41:29.363283Z",
     "shell.execute_reply.started": "2025-07-26T18:41:29.358185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "args.model_type = \"llama\"  # will be inferred automatically\n",
    "args.sparsity_ratio = 0.2\n",
    "args.nsamples = 2\n",
    "args.sparsity_type = 'unstructured'\n",
    "args.prune_method = 'DSnoT' #choices=[\"wanda\", \"sparsegpt\", \"magnitude\", \"DSnoT\"]\n",
    "args.initial_method = 'sparsegpt' # options = [magnitude, wanda, sparsegpt]\n",
    "args.max_cycle_time =50\n",
    "args.update_threshold=0.1\n",
    "args.pow_of_var_regrowing=1\n",
    "args.output_results_file = 'llama-3b-DSnoT-results.txt'\n",
    "args.cache_dir = 'llama-3b-weights'\n",
    "args.save_model = \"pruned_DSnoT_llama_3b\"\n",
    "args.seed = 0\n",
    "args.max_cycle_time = 10\n",
    "args.update_threshold = 0.1\n",
    "args.pow_of_var_regrowing = 1\n",
    "args.pow_of_var_pruning = 1\n",
    "args.skip_layer = \"no_skip\"\n",
    "args.skip_sub_layer = \"no_skip\"\n",
    "args.without_DSnoT = False\n",
    "args.without_same_sign = \"True\"\n",
    "args.get_time_overhead = False\n",
    "args.output_results_file = \"test_results.txt\"\n",
    "args.save_path= \"pruned_llama3b_instruct_dsnot_with_sparsegpt\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:41:31.630353Z",
     "iopub.status.busy": "2025-07-26T18:41:31.629707Z",
     "iopub.status.idle": "2025-07-26T18:43:23.188038Z",
     "shell.execute_reply": "2025-07-26T18:43:23.187410Z",
     "shell.execute_reply.started": "2025-07-26T18:41:31.630330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: llama\n",
      "loading llm model meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c1a6315c744f3e8ca93d39916b19b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 sparsity 0.000002\n",
      "layer 1 sparsity 0.000001\n",
      "layer 2 sparsity 0.000001\n",
      "layer 3 sparsity 0.000001\n",
      "layer 4 sparsity 0.000001\n",
      "layer 5 sparsity 0.000002\n",
      "layer 6 sparsity 0.000001\n",
      "layer 7 sparsity 0.000001\n",
      "layer 8 sparsity 0.000001\n",
      "layer 9 sparsity 0.000001\n",
      "layer 10 sparsity 0.000001\n",
      "layer 11 sparsity 0.000002\n",
      "layer 12 sparsity 0.000002\n",
      "layer 13 sparsity 0.000002\n",
      "layer 14 sparsity 0.000002\n",
      "layer 15 sparsity 0.000001\n",
      "layer 16 sparsity 0.000001\n",
      "layer 17 sparsity 0.000001\n",
      "layer 18 sparsity 0.000001\n",
      "layer 19 sparsity 0.000001\n",
      "layer 20 sparsity 0.000001\n",
      "layer 21 sparsity 0.000001\n",
      "layer 22 sparsity 0.000001\n",
      "layer 23 sparsity 0.000001\n",
      "layer 24 sparsity 0.000001\n",
      "layer 25 sparsity 0.000001\n",
      "layer 26 sparsity 0.000001\n",
      "layer 27 sparsity 0.000001\n",
      "Initial sparsity: 0.0000\n",
      "Initial parameters: 3212.75M\n",
      "Using device: cuda:0\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Sparsity ratio: 0.2\n",
      "Samples: 2\n",
      "pruning starts\n",
      "Starting DSNOT PRUNING....\n",
      "Original model parameters: 3212.75M\n",
      "loading calibdation data\n",
      "[INFO] Loading training and validation datasets...\n",
      "[INFO] Datasets loaded successfully.\n",
      "[INFO] Starting to generate 2 training samples...\n",
      "[DEBUG] Sample 1 added. inp shape: torch.Size([1, 2048]), tar shape: torch.Size([1, 2048])\n",
      "[DEBUG] Sample 2 added. inp shape: torch.Size([1, 2048]), tar shape: torch.Size([1, 2048])\n",
      "[INFO] All training samples generated successfully.\n",
      "[INFO] Preparing validation dataset...\n",
      "dataset loading complete\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 0 name self_attn.q_proj\n",
      "pruning layer 0 name self_attn.k_proj\n",
      "pruning layer 0 name self_attn.v_proj\n",
      "pruning layer 0 name self_attn.o_proj\n",
      "pruning layer 0 name mlp.gate_proj\n",
      "pruning layer 0 name mlp.up_proj\n",
      "pruning layer 0 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 1 name self_attn.q_proj\n",
      "pruning layer 1 name self_attn.k_proj\n",
      "pruning layer 1 name self_attn.v_proj\n",
      "pruning layer 1 name self_attn.o_proj\n",
      "pruning layer 1 name mlp.gate_proj\n",
      "pruning layer 1 name mlp.up_proj\n",
      "pruning layer 1 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 2 name self_attn.q_proj\n",
      "pruning layer 2 name self_attn.k_proj\n",
      "pruning layer 2 name self_attn.v_proj\n",
      "pruning layer 2 name self_attn.o_proj\n",
      "pruning layer 2 name mlp.gate_proj\n",
      "pruning layer 2 name mlp.up_proj\n",
      "pruning layer 2 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 3 name self_attn.q_proj\n",
      "pruning layer 3 name self_attn.k_proj\n",
      "pruning layer 3 name self_attn.v_proj\n",
      "pruning layer 3 name self_attn.o_proj\n",
      "pruning layer 3 name mlp.gate_proj\n",
      "pruning layer 3 name mlp.up_proj\n",
      "pruning layer 3 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 4 name self_attn.q_proj\n",
      "pruning layer 4 name self_attn.k_proj\n",
      "pruning layer 4 name self_attn.v_proj\n",
      "pruning layer 4 name self_attn.o_proj\n",
      "pruning layer 4 name mlp.gate_proj\n",
      "pruning layer 4 name mlp.up_proj\n",
      "pruning layer 4 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 5 name self_attn.q_proj\n",
      "pruning layer 5 name self_attn.k_proj\n",
      "pruning layer 5 name self_attn.v_proj\n",
      "pruning layer 5 name self_attn.o_proj\n",
      "pruning layer 5 name mlp.gate_proj\n",
      "pruning layer 5 name mlp.up_proj\n",
      "pruning layer 5 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 6 name self_attn.q_proj\n",
      "pruning layer 6 name self_attn.k_proj\n",
      "pruning layer 6 name self_attn.v_proj\n",
      "pruning layer 6 name self_attn.o_proj\n",
      "pruning layer 6 name mlp.gate_proj\n",
      "pruning layer 6 name mlp.up_proj\n",
      "pruning layer 6 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 7 name self_attn.q_proj\n",
      "pruning layer 7 name self_attn.k_proj\n",
      "pruning layer 7 name self_attn.v_proj\n",
      "pruning layer 7 name self_attn.o_proj\n",
      "pruning layer 7 name mlp.gate_proj\n",
      "pruning layer 7 name mlp.up_proj\n",
      "pruning layer 7 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 8 name self_attn.q_proj\n",
      "pruning layer 8 name self_attn.k_proj\n",
      "pruning layer 8 name self_attn.v_proj\n",
      "pruning layer 8 name self_attn.o_proj\n",
      "pruning layer 8 name mlp.gate_proj\n",
      "pruning layer 8 name mlp.up_proj\n",
      "pruning layer 8 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 9 name self_attn.q_proj\n",
      "pruning layer 9 name self_attn.k_proj\n",
      "pruning layer 9 name self_attn.v_proj\n",
      "pruning layer 9 name self_attn.o_proj\n",
      "pruning layer 9 name mlp.gate_proj\n",
      "pruning layer 9 name mlp.up_proj\n",
      "pruning layer 9 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 10 name self_attn.q_proj\n",
      "pruning layer 10 name self_attn.k_proj\n",
      "pruning layer 10 name self_attn.v_proj\n",
      "pruning layer 10 name self_attn.o_proj\n",
      "pruning layer 10 name mlp.gate_proj\n",
      "pruning layer 10 name mlp.up_proj\n",
      "pruning layer 10 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 11 name self_attn.q_proj\n",
      "pruning layer 11 name self_attn.k_proj\n",
      "pruning layer 11 name self_attn.v_proj\n",
      "pruning layer 11 name self_attn.o_proj\n",
      "pruning layer 11 name mlp.gate_proj\n",
      "pruning layer 11 name mlp.up_proj\n",
      "pruning layer 11 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 12 name self_attn.q_proj\n",
      "pruning layer 12 name self_attn.k_proj\n",
      "pruning layer 12 name self_attn.v_proj\n",
      "pruning layer 12 name self_attn.o_proj\n",
      "pruning layer 12 name mlp.gate_proj\n",
      "pruning layer 12 name mlp.up_proj\n",
      "pruning layer 12 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 13 name self_attn.q_proj\n",
      "pruning layer 13 name self_attn.k_proj\n",
      "pruning layer 13 name self_attn.v_proj\n",
      "pruning layer 13 name self_attn.o_proj\n",
      "pruning layer 13 name mlp.gate_proj\n",
      "pruning layer 13 name mlp.up_proj\n",
      "pruning layer 13 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 14 name self_attn.q_proj\n",
      "pruning layer 14 name self_attn.k_proj\n",
      "pruning layer 14 name self_attn.v_proj\n",
      "pruning layer 14 name self_attn.o_proj\n",
      "pruning layer 14 name mlp.gate_proj\n",
      "pruning layer 14 name mlp.up_proj\n",
      "pruning layer 14 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 15 name self_attn.q_proj\n",
      "pruning layer 15 name self_attn.k_proj\n",
      "pruning layer 15 name self_attn.v_proj\n",
      "pruning layer 15 name self_attn.o_proj\n",
      "pruning layer 15 name mlp.gate_proj\n",
      "pruning layer 15 name mlp.up_proj\n",
      "pruning layer 15 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 16 name self_attn.q_proj\n",
      "pruning layer 16 name self_attn.k_proj\n",
      "pruning layer 16 name self_attn.v_proj\n",
      "pruning layer 16 name self_attn.o_proj\n",
      "pruning layer 16 name mlp.gate_proj\n",
      "pruning layer 16 name mlp.up_proj\n",
      "pruning layer 16 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 17 name self_attn.q_proj\n",
      "pruning layer 17 name self_attn.k_proj\n",
      "pruning layer 17 name self_attn.v_proj\n",
      "pruning layer 17 name self_attn.o_proj\n",
      "pruning layer 17 name mlp.gate_proj\n",
      "pruning layer 17 name mlp.up_proj\n",
      "pruning layer 17 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 18 name self_attn.q_proj\n",
      "pruning layer 18 name self_attn.k_proj\n",
      "pruning layer 18 name self_attn.v_proj\n",
      "pruning layer 18 name self_attn.o_proj\n",
      "pruning layer 18 name mlp.gate_proj\n",
      "pruning layer 18 name mlp.up_proj\n",
      "pruning layer 18 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 19 name self_attn.q_proj\n",
      "pruning layer 19 name self_attn.k_proj\n",
      "pruning layer 19 name self_attn.v_proj\n",
      "pruning layer 19 name self_attn.o_proj\n",
      "pruning layer 19 name mlp.gate_proj\n",
      "pruning layer 19 name mlp.up_proj\n",
      "pruning layer 19 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 20 name self_attn.q_proj\n",
      "pruning layer 20 name self_attn.k_proj\n",
      "pruning layer 20 name self_attn.v_proj\n",
      "pruning layer 20 name self_attn.o_proj\n",
      "pruning layer 20 name mlp.gate_proj\n",
      "pruning layer 20 name mlp.up_proj\n",
      "pruning layer 20 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 21 name self_attn.q_proj\n",
      "pruning layer 21 name self_attn.k_proj\n",
      "pruning layer 21 name self_attn.v_proj\n",
      "pruning layer 21 name self_attn.o_proj\n",
      "pruning layer 21 name mlp.gate_proj\n",
      "pruning layer 21 name mlp.up_proj\n",
      "pruning layer 21 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 22 name self_attn.q_proj\n",
      "pruning layer 22 name self_attn.k_proj\n",
      "pruning layer 22 name self_attn.v_proj\n",
      "pruning layer 22 name self_attn.o_proj\n",
      "pruning layer 22 name mlp.gate_proj\n",
      "pruning layer 22 name mlp.up_proj\n",
      "pruning layer 22 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 23 name self_attn.q_proj\n",
      "pruning layer 23 name self_attn.k_proj\n",
      "pruning layer 23 name self_attn.v_proj\n",
      "pruning layer 23 name self_attn.o_proj\n",
      "pruning layer 23 name mlp.gate_proj\n",
      "pruning layer 23 name mlp.up_proj\n",
      "pruning layer 23 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 24 name self_attn.q_proj\n",
      "pruning layer 24 name self_attn.k_proj\n",
      "pruning layer 24 name self_attn.v_proj\n",
      "pruning layer 24 name self_attn.o_proj\n",
      "pruning layer 24 name mlp.gate_proj\n",
      "pruning layer 24 name mlp.up_proj\n",
      "pruning layer 24 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 25 name self_attn.q_proj\n",
      "pruning layer 25 name self_attn.k_proj\n",
      "pruning layer 25 name self_attn.v_proj\n",
      "pruning layer 25 name self_attn.o_proj\n",
      "pruning layer 25 name mlp.gate_proj\n",
      "pruning layer 25 name mlp.up_proj\n",
      "pruning layer 25 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 26 name self_attn.q_proj\n",
      "pruning layer 26 name self_attn.k_proj\n",
      "pruning layer 26 name self_attn.v_proj\n",
      "pruning layer 26 name self_attn.o_proj\n",
      "pruning layer 26 name mlp.gate_proj\n",
      "pruning layer 26 name mlp.up_proj\n",
      "pruning layer 26 name mlp.down_proj\n",
      "This was the condition of LLama-30B and Llama-65B and is now removed...\n",
      "Running forward passes for calibration...\n",
      "pruning layer 27 name self_attn.q_proj\n",
      "pruning layer 27 name self_attn.k_proj\n",
      "pruning layer 27 name self_attn.v_proj\n",
      "pruning layer 27 name self_attn.o_proj\n",
      "pruning layer 27 name mlp.gate_proj\n",
      "pruning layer 27 name mlp.up_proj\n",
      "pruning layer 27 name mlp.down_proj\n",
      "Validating parameter count after full pruning and reconfiguration...\n",
      "✅ Final Pruned Model Parameters: 3212.75M\n",
      "Compression ratio: 0.00%\n",
      "Saving pruned model to pruned_llama3b_instruct_dsnot_with_sparsegpt...\n",
      "Pruned model saved successfully to pruned_llama3b_instruct_dsnot_with_sparsegpt\n",
      "******************************\n",
      "layer 0 sparsity 0.199890\n",
      "layer 1 sparsity 0.199890\n",
      "layer 2 sparsity 0.199890\n",
      "layer 3 sparsity 0.199890\n",
      "layer 4 sparsity 0.199890\n",
      "layer 5 sparsity 0.199890\n",
      "layer 6 sparsity 0.199890\n",
      "layer 7 sparsity 0.199890\n",
      "layer 8 sparsity 0.199890\n",
      "layer 9 sparsity 0.199890\n",
      "layer 10 sparsity 0.199890\n",
      "layer 11 sparsity 0.199890\n",
      "layer 12 sparsity 0.199890\n",
      "layer 13 sparsity 0.199890\n",
      "layer 14 sparsity 0.199890\n",
      "layer 15 sparsity 0.199890\n",
      "layer 16 sparsity 0.199890\n",
      "layer 17 sparsity 0.199890\n",
      "layer 18 sparsity 0.199890\n",
      "layer 19 sparsity 0.199890\n",
      "layer 20 sparsity 0.199890\n",
      "layer 21 sparsity 0.199890\n",
      "layer 22 sparsity 0.199890\n",
      "layer 23 sparsity 0.199890\n",
      "layer 24 sparsity 0.199890\n",
      "layer 25 sparsity 0.199890\n",
      "layer 26 sparsity 0.199890\n",
      "layer 27 sparsity 0.199890\n",
      "sparsity sanity check 0.1999\n",
      "******************************\n",
      "Final parameters: 3212.75M\n",
      "Compression ratio: 0.00%\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING WANDA BASED PRUNING ON LLAMA 3B (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T17:24:49.579953Z",
     "iopub.status.busy": "2025-07-26T17:24:49.579651Z",
     "iopub.status.idle": "2025-07-26T17:24:49.585334Z",
     "shell.execute_reply": "2025-07-26T17:24:49.584559Z",
     "shell.execute_reply.started": "2025-07-26T17:24:49.579934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "args.model_type = \"llama\"  # will be inferred automatically\n",
    "args.sparsity_ratio = 0.2\n",
    "args.nsamples = 2\n",
    "args.sparsity_type = 'unstructured'\n",
    "args.prune_method = 'wanda' #choices=[\"wanda\", \"sparsegpt\", \"magnitude\", \"DSnoT\"]\n",
    "args.max_cycle_time =50\n",
    "args.update_threshold=0.1\n",
    "args.pow_of_var_regrowing=1\n",
    "args.output_results_file = 'llama-3b-wanda-results.txt'\n",
    "args.cache_dir = 'llama-3b-weights'\n",
    "args.save_model = \"pruned_wanda_llama_3b\"\n",
    "args.seed = 0\n",
    "args.max_cycle_time = 10\n",
    "args.update_threshold = 0.1\n",
    "args.pow_of_var_regrowing = 1\n",
    "args.pow_of_var_pruning = 1\n",
    "args.skip_layer = \"no_skip\"\n",
    "args.skip_sub_layer = \"no_skip\"\n",
    "args.without_DSnoT = True\n",
    "args.without_same_sign = \"True\"\n",
    "args.get_time_overhead = False\n",
    "args.output_results_file = \"test_results.txt\"\n",
    "args.save_path=\"SAVE_PATH_FOR_YOUR_MODEL_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-26T17:24:53.562485Z",
     "iopub.status.busy": "2025-07-26T17:24:53.562183Z",
     "iopub.status.idle": "2025-07-26T17:26:26.128582Z",
     "shell.execute_reply": "2025-07-26T17:26:26.127482Z",
     "shell.execute_reply.started": "2025-07-26T17:24:53.562463Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: llama\n",
      "loading llm model meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60117cc69bad411ea2d6ee38805bc225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 17:24:55.561159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753550695.764210      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753550695.817618      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686408fa09a14627ae895f5ef0c635f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67265657e2344855ae2d69444d575ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c41a54120e45089e395d0307764187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec222b3f6bf487193cf647e0d6ffb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02124f7485504984bd78295c3a62706a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e31befc9006401c9322df761f485ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b9ef93e12144058e1c2e1a04470f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b07f9b4a6840fe94bbf13bfe3fddc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1432b4ec274e2694c9a6ec9cee6673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 sparsity 0.000002\n",
      "layer 1 sparsity 0.000001\n",
      "layer 2 sparsity 0.000001\n",
      "layer 3 sparsity 0.000001\n",
      "layer 4 sparsity 0.000001\n",
      "layer 5 sparsity 0.000002\n",
      "layer 6 sparsity 0.000001\n",
      "layer 7 sparsity 0.000001\n",
      "layer 8 sparsity 0.000001\n",
      "layer 9 sparsity 0.000001\n",
      "layer 10 sparsity 0.000001\n",
      "layer 11 sparsity 0.000002\n",
      "layer 12 sparsity 0.000002\n",
      "layer 13 sparsity 0.000002\n",
      "layer 14 sparsity 0.000002\n",
      "layer 15 sparsity 0.000001\n",
      "layer 16 sparsity 0.000001\n",
      "layer 17 sparsity 0.000001\n",
      "layer 18 sparsity 0.000001\n",
      "layer 19 sparsity 0.000001\n",
      "layer 20 sparsity 0.000001\n",
      "layer 21 sparsity 0.000001\n",
      "layer 22 sparsity 0.000001\n",
      "layer 23 sparsity 0.000001\n",
      "layer 24 sparsity 0.000001\n",
      "layer 25 sparsity 0.000001\n",
      "layer 26 sparsity 0.000001\n",
      "layer 27 sparsity 0.000001\n",
      "Initial sparsity: 0.0000\n",
      "Initial parameters: 3212.75M\n",
      "Using device: cuda:0\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Sparsity ratio: 0.2\n",
      "Samples: 2\n",
      "pruning starts\n",
      "loading calibdation data\n",
      "[INFO] Loading training and validation datasets...\n",
      "[INFO] Datasets loaded successfully.\n",
      "[INFO] Starting to generate 2 training samples...\n",
      "[DEBUG] Sample 1 added. inp shape: torch.Size([1, 2048]), tar shape: torch.Size([1, 2048])\n",
      "[DEBUG] Sample 2 added. inp shape: torch.Size([1, 2048]), tar shape: torch.Size([1, 2048])\n",
      "[INFO] All training samples generated successfully.\n",
      "[INFO] Preparing validation dataset...\n",
      "dataset loading complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning layer 0 name self_attn.q_proj\n",
      "pruning layer 0 name self_attn.k_proj\n",
      "pruning layer 0 name self_attn.v_proj\n",
      "pruning layer 0 name self_attn.o_proj\n",
      "pruning layer 0 name mlp.gate_proj\n",
      "pruning layer 0 name mlp.up_proj\n",
      "pruning layer 0 name mlp.down_proj\n",
      "pruning layer 1 name self_attn.q_proj\n",
      "pruning layer 1 name self_attn.k_proj\n",
      "pruning layer 1 name self_attn.v_proj\n",
      "pruning layer 1 name self_attn.o_proj\n",
      "pruning layer 1 name mlp.gate_proj\n",
      "pruning layer 1 name mlp.up_proj\n",
      "pruning layer 1 name mlp.down_proj\n",
      "pruning layer 2 name self_attn.q_proj\n",
      "pruning layer 2 name self_attn.k_proj\n",
      "pruning layer 2 name self_attn.v_proj\n",
      "pruning layer 2 name self_attn.o_proj\n",
      "pruning layer 2 name mlp.gate_proj\n",
      "pruning layer 2 name mlp.up_proj\n",
      "pruning layer 2 name mlp.down_proj\n",
      "pruning layer 3 name self_attn.q_proj\n",
      "pruning layer 3 name self_attn.k_proj\n",
      "pruning layer 3 name self_attn.v_proj\n",
      "pruning layer 3 name self_attn.o_proj\n",
      "pruning layer 3 name mlp.gate_proj\n",
      "pruning layer 3 name mlp.up_proj\n",
      "pruning layer 3 name mlp.down_proj\n",
      "pruning layer 4 name self_attn.q_proj\n",
      "pruning layer 4 name self_attn.k_proj\n",
      "pruning layer 4 name self_attn.v_proj\n",
      "pruning layer 4 name self_attn.o_proj\n",
      "pruning layer 4 name mlp.gate_proj\n",
      "pruning layer 4 name mlp.up_proj\n",
      "pruning layer 4 name mlp.down_proj\n",
      "pruning layer 5 name self_attn.q_proj\n",
      "pruning layer 5 name self_attn.k_proj\n",
      "pruning layer 5 name self_attn.v_proj\n",
      "pruning layer 5 name self_attn.o_proj\n",
      "pruning layer 5 name mlp.gate_proj\n",
      "pruning layer 5 name mlp.up_proj\n",
      "pruning layer 5 name mlp.down_proj\n",
      "pruning layer 6 name self_attn.q_proj\n",
      "pruning layer 6 name self_attn.k_proj\n",
      "pruning layer 6 name self_attn.v_proj\n",
      "pruning layer 6 name self_attn.o_proj\n",
      "pruning layer 6 name mlp.gate_proj\n",
      "pruning layer 6 name mlp.up_proj\n",
      "pruning layer 6 name mlp.down_proj\n",
      "pruning layer 7 name self_attn.q_proj\n",
      "pruning layer 7 name self_attn.k_proj\n",
      "pruning layer 7 name self_attn.v_proj\n",
      "pruning layer 7 name self_attn.o_proj\n",
      "pruning layer 7 name mlp.gate_proj\n",
      "pruning layer 7 name mlp.up_proj\n",
      "pruning layer 7 name mlp.down_proj\n",
      "pruning layer 8 name self_attn.q_proj\n",
      "pruning layer 8 name self_attn.k_proj\n",
      "pruning layer 8 name self_attn.v_proj\n",
      "pruning layer 8 name self_attn.o_proj\n",
      "pruning layer 8 name mlp.gate_proj\n",
      "pruning layer 8 name mlp.up_proj\n",
      "pruning layer 8 name mlp.down_proj\n",
      "pruning layer 9 name self_attn.q_proj\n",
      "pruning layer 9 name self_attn.k_proj\n",
      "pruning layer 9 name self_attn.v_proj\n",
      "pruning layer 9 name self_attn.o_proj\n",
      "pruning layer 9 name mlp.gate_proj\n",
      "pruning layer 9 name mlp.up_proj\n",
      "pruning layer 9 name mlp.down_proj\n",
      "pruning layer 10 name self_attn.q_proj\n",
      "pruning layer 10 name self_attn.k_proj\n",
      "pruning layer 10 name self_attn.v_proj\n",
      "pruning layer 10 name self_attn.o_proj\n",
      "pruning layer 10 name mlp.gate_proj\n",
      "pruning layer 10 name mlp.up_proj\n",
      "pruning layer 10 name mlp.down_proj\n",
      "pruning layer 11 name self_attn.q_proj\n",
      "pruning layer 11 name self_attn.k_proj\n",
      "pruning layer 11 name self_attn.v_proj\n",
      "pruning layer 11 name self_attn.o_proj\n",
      "pruning layer 11 name mlp.gate_proj\n",
      "pruning layer 11 name mlp.up_proj\n",
      "pruning layer 11 name mlp.down_proj\n",
      "pruning layer 12 name self_attn.q_proj\n",
      "pruning layer 12 name self_attn.k_proj\n",
      "pruning layer 12 name self_attn.v_proj\n",
      "pruning layer 12 name self_attn.o_proj\n",
      "pruning layer 12 name mlp.gate_proj\n",
      "pruning layer 12 name mlp.up_proj\n",
      "pruning layer 12 name mlp.down_proj\n",
      "pruning layer 13 name self_attn.q_proj\n",
      "pruning layer 13 name self_attn.k_proj\n",
      "pruning layer 13 name self_attn.v_proj\n",
      "pruning layer 13 name self_attn.o_proj\n",
      "pruning layer 13 name mlp.gate_proj\n",
      "pruning layer 13 name mlp.up_proj\n",
      "pruning layer 13 name mlp.down_proj\n",
      "pruning layer 14 name self_attn.q_proj\n",
      "pruning layer 14 name self_attn.k_proj\n",
      "pruning layer 14 name self_attn.v_proj\n",
      "pruning layer 14 name self_attn.o_proj\n",
      "pruning layer 14 name mlp.gate_proj\n",
      "pruning layer 14 name mlp.up_proj\n",
      "pruning layer 14 name mlp.down_proj\n",
      "pruning layer 15 name self_attn.q_proj\n",
      "pruning layer 15 name self_attn.k_proj\n",
      "pruning layer 15 name self_attn.v_proj\n",
      "pruning layer 15 name self_attn.o_proj\n",
      "pruning layer 15 name mlp.gate_proj\n",
      "pruning layer 15 name mlp.up_proj\n",
      "pruning layer 15 name mlp.down_proj\n",
      "pruning layer 16 name self_attn.q_proj\n",
      "pruning layer 16 name self_attn.k_proj\n",
      "pruning layer 16 name self_attn.v_proj\n",
      "pruning layer 16 name self_attn.o_proj\n",
      "pruning layer 16 name mlp.gate_proj\n",
      "pruning layer 16 name mlp.up_proj\n",
      "pruning layer 16 name mlp.down_proj\n",
      "pruning layer 17 name self_attn.q_proj\n",
      "pruning layer 17 name self_attn.k_proj\n",
      "pruning layer 17 name self_attn.v_proj\n",
      "pruning layer 17 name self_attn.o_proj\n",
      "pruning layer 17 name mlp.gate_proj\n",
      "pruning layer 17 name mlp.up_proj\n",
      "pruning layer 17 name mlp.down_proj\n",
      "pruning layer 18 name self_attn.q_proj\n",
      "pruning layer 18 name self_attn.k_proj\n",
      "pruning layer 18 name self_attn.v_proj\n",
      "pruning layer 18 name self_attn.o_proj\n",
      "pruning layer 18 name mlp.gate_proj\n",
      "pruning layer 18 name mlp.up_proj\n",
      "pruning layer 18 name mlp.down_proj\n",
      "pruning layer 19 name self_attn.q_proj\n",
      "pruning layer 19 name self_attn.k_proj\n",
      "pruning layer 19 name self_attn.v_proj\n",
      "pruning layer 19 name self_attn.o_proj\n",
      "pruning layer 19 name mlp.gate_proj\n",
      "pruning layer 19 name mlp.up_proj\n",
      "pruning layer 19 name mlp.down_proj\n",
      "pruning layer 20 name self_attn.q_proj\n",
      "pruning layer 20 name self_attn.k_proj\n",
      "pruning layer 20 name self_attn.v_proj\n",
      "pruning layer 20 name self_attn.o_proj\n",
      "pruning layer 20 name mlp.gate_proj\n",
      "pruning layer 20 name mlp.up_proj\n",
      "pruning layer 20 name mlp.down_proj\n",
      "pruning layer 21 name self_attn.q_proj\n",
      "pruning layer 21 name self_attn.k_proj\n",
      "pruning layer 21 name self_attn.v_proj\n",
      "pruning layer 21 name self_attn.o_proj\n",
      "pruning layer 21 name mlp.gate_proj\n",
      "pruning layer 21 name mlp.up_proj\n",
      "pruning layer 21 name mlp.down_proj\n",
      "pruning layer 22 name self_attn.q_proj\n",
      "pruning layer 22 name self_attn.k_proj\n",
      "pruning layer 22 name self_attn.v_proj\n",
      "pruning layer 22 name self_attn.o_proj\n",
      "pruning layer 22 name mlp.gate_proj\n",
      "pruning layer 22 name mlp.up_proj\n",
      "pruning layer 22 name mlp.down_proj\n",
      "pruning layer 23 name self_attn.q_proj\n",
      "pruning layer 23 name self_attn.k_proj\n",
      "pruning layer 23 name self_attn.v_proj\n",
      "pruning layer 23 name self_attn.o_proj\n",
      "pruning layer 23 name mlp.gate_proj\n",
      "pruning layer 23 name mlp.up_proj\n",
      "pruning layer 23 name mlp.down_proj\n",
      "pruning layer 24 name self_attn.q_proj\n",
      "pruning layer 24 name self_attn.k_proj\n",
      "pruning layer 24 name self_attn.v_proj\n",
      "pruning layer 24 name self_attn.o_proj\n",
      "pruning layer 24 name mlp.gate_proj\n",
      "pruning layer 24 name mlp.up_proj\n",
      "pruning layer 24 name mlp.down_proj\n",
      "pruning layer 25 name self_attn.q_proj\n",
      "pruning layer 25 name self_attn.k_proj\n",
      "pruning layer 25 name self_attn.v_proj\n",
      "pruning layer 25 name self_attn.o_proj\n",
      "pruning layer 25 name mlp.gate_proj\n",
      "pruning layer 25 name mlp.up_proj\n",
      "pruning layer 25 name mlp.down_proj\n",
      "pruning layer 26 name self_attn.q_proj\n",
      "pruning layer 26 name self_attn.k_proj\n",
      "pruning layer 26 name self_attn.v_proj\n",
      "pruning layer 26 name self_attn.o_proj\n",
      "pruning layer 26 name mlp.gate_proj\n",
      "pruning layer 26 name mlp.up_proj\n",
      "pruning layer 26 name mlp.down_proj\n",
      "pruning layer 27 name self_attn.q_proj\n",
      "pruning layer 27 name self_attn.k_proj\n",
      "pruning layer 27 name self_attn.v_proj\n",
      "pruning layer 27 name self_attn.o_proj\n",
      "pruning layer 27 name mlp.gate_proj\n",
      "pruning layer 27 name mlp.up_proj\n",
      "pruning layer 27 name mlp.down_proj\n",
      "Validating parameter count after full pruning and reconfiguration...\n",
      "✅ Final Pruned Model Parameters: 3212.75M\n",
      "Compression ratio: 0.00%\n",
      "******************************\n",
      "layer 0 sparsity 0.199890\n",
      "layer 1 sparsity 0.199890\n",
      "layer 2 sparsity 0.199890\n",
      "layer 3 sparsity 0.199890\n",
      "layer 4 sparsity 0.199890\n",
      "layer 5 sparsity 0.199890\n",
      "layer 6 sparsity 0.199890\n",
      "layer 7 sparsity 0.199890\n",
      "layer 8 sparsity 0.199890\n",
      "layer 9 sparsity 0.199890\n",
      "layer 10 sparsity 0.199890\n",
      "layer 11 sparsity 0.199890\n",
      "layer 12 sparsity 0.199890\n",
      "layer 13 sparsity 0.199890\n",
      "layer 14 sparsity 0.199890\n",
      "layer 15 sparsity 0.199890\n",
      "layer 16 sparsity 0.199890\n",
      "layer 17 sparsity 0.199890\n",
      "layer 18 sparsity 0.199890\n",
      "layer 19 sparsity 0.199890\n",
      "layer 20 sparsity 0.199890\n",
      "layer 21 sparsity 0.199890\n",
      "layer 22 sparsity 0.199890\n",
      "layer 23 sparsity 0.199890\n",
      "layer 24 sparsity 0.199890\n",
      "layer 25 sparsity 0.199890\n",
      "layer 26 sparsity 0.199890\n",
      "layer 27 sparsity 0.199890\n",
      "sparsity sanity check 0.1999\n",
      "******************************\n",
      "Final parameters: 3212.75M\n",
      "Compression ratio: 0.00%\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING SparseGPT PRUNING ON LLAMA 3B (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T17:54:38.708665Z",
     "iopub.status.busy": "2025-07-26T17:54:38.708350Z",
     "iopub.status.idle": "2025-07-26T17:54:38.714440Z",
     "shell.execute_reply": "2025-07-26T17:54:38.713543Z",
     "shell.execute_reply.started": "2025-07-26T17:54:38.708645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "args.model_type = \"llama\"  # will be inferred automatically\n",
    "args.sparsity_ratio = 0.2\n",
    "args.nsamples = 2\n",
    "args.sparsity_type = 'unstructured'\n",
    "args.prune_method = 'sparsegpt' #choices=[\"wanda\", \"sparsegpt\", \"magnitude\", \"DSnoT\"]\n",
    "args.max_cycle_time =50\n",
    "args.update_threshold=0.1\n",
    "args.pow_of_var_regrowing=1\n",
    "args.output_results_file = 'llama-3b-sparsegpt-results.txt'\n",
    "args.cache_dir = 'llama-3b-weights'\n",
    "args.save_model = \"pruned_sparsegpt_llama_3b\"\n",
    "args.seed = 0\n",
    "args.max_cycle_time = 10\n",
    "args.update_threshold = 0.1\n",
    "args.pow_of_var_regrowing = 1\n",
    "args.pow_of_var_pruning = 1\n",
    "args.skip_layer = \"no_skip\"\n",
    "args.skip_sub_layer = \"no_skip\"\n",
    "args.without_same_sign = \"True\"\n",
    "args.get_time_overhead = False\n",
    "args.output_results_file = \"test_results.txt\"\n",
    "args.save_path = \"pruned-llama-3b_instruct_sparsegpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-26T17:54:40.768009Z",
     "iopub.status.busy": "2025-07-26T17:54:40.767404Z",
     "iopub.status.idle": "2025-07-26T17:58:50.769710Z",
     "shell.execute_reply": "2025-07-26T17:58:50.769011Z",
     "shell.execute_reply.started": "2025-07-26T17:54:40.767981Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: llama\n",
      "loading llm model meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c871a3303de4efeb60bbe0cfd247448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 sparsity 0.000002\n",
      "layer 1 sparsity 0.000001\n",
      "layer 2 sparsity 0.000001\n",
      "layer 3 sparsity 0.000001\n",
      "layer 4 sparsity 0.000001\n",
      "layer 5 sparsity 0.000002\n",
      "layer 6 sparsity 0.000001\n",
      "layer 7 sparsity 0.000001\n",
      "layer 8 sparsity 0.000001\n",
      "layer 9 sparsity 0.000001\n",
      "layer 10 sparsity 0.000001\n",
      "layer 11 sparsity 0.000002\n",
      "layer 12 sparsity 0.000002\n",
      "layer 13 sparsity 0.000002\n",
      "layer 14 sparsity 0.000002\n",
      "layer 15 sparsity 0.000001\n",
      "layer 16 sparsity 0.000001\n",
      "layer 17 sparsity 0.000001\n",
      "layer 18 sparsity 0.000001\n",
      "layer 19 sparsity 0.000001\n",
      "layer 20 sparsity 0.000001\n",
      "layer 21 sparsity 0.000001\n",
      "layer 22 sparsity 0.000001\n",
      "layer 23 sparsity 0.000001\n",
      "layer 24 sparsity 0.000001\n",
      "layer 25 sparsity 0.000001\n",
      "layer 26 sparsity 0.000001\n",
      "layer 27 sparsity 0.000001\n",
      "Initial sparsity: 0.0000\n",
      "Initial parameters: 3212.75M\n",
      "Using device: cuda:0\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Sparsity ratio: 0.2\n",
      "Samples: 2\n",
      "pruning starts\n",
      "Starting pruning process...\n",
      "Original model parameters: 3212.75M\n",
      "Loading dataset...\n",
      "[INFO] Loading training and validation datasets...\n",
      "[INFO] Datasets loaded successfully.\n",
      "[INFO] Starting to generate 2 training samples...\n",
      "[DEBUG] Sample 1 added. inp shape: torch.Size([1, 2048]), tar shape: torch.Size([1, 2048])\n",
      "[DEBUG] Sample 2 added. inp shape: torch.Size([1, 2048]), tar shape: torch.Size([1, 2048])\n",
      "[INFO] All training samples generated successfully.\n",
      "[INFO] Preparing validation dataset...\n",
      "Dataset loaded.\n",
      "Total layers to prune: 28\n",
      "Initialized input tensor.\n",
      "Catcher injected into first layer.\n",
      "Feeding data to model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catcher forward pass: sample 0\n",
      "Catcher forward pass: sample 1\n",
      "Data fed. Removing Catcher.\n",
      "Cache collected. Starting pruning.\n",
      "\n",
      "--- Pruning Layer 0 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 0, Sample 0\n",
      "Layer 0, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 0\n",
      "Pruned self_attn.q_proj in 1.26s\n",
      "Pruning sublayer self_attn.k_proj in layer 0\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 0\n",
      "Pruned self_attn.v_proj in 0.77s\n",
      "Pruning sublayer self_attn.o_proj in layer 0\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 0\n",
      "Pruned mlp.gate_proj in 0.74s\n",
      "Pruning sublayer mlp.up_proj in layer 0\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 0\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 0 with forward pass...\n",
      "Finished Layer 0\n",
      "\n",
      "--- Pruning Layer 1 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 1, Sample 0\n",
      "Layer 1, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 1\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 1\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 1\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 1\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 1\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 1\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 1\n",
      "Pruned mlp.down_proj in 2.24s\n",
      "Validating pruned layer 1 with forward pass...\n",
      "Finished Layer 1\n",
      "\n",
      "--- Pruning Layer 2 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 2, Sample 0\n",
      "Layer 2, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 2\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 2\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 2\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 2\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 2\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 2\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 2\n",
      "Pruned mlp.down_proj in 2.12s\n",
      "Validating pruned layer 2 with forward pass...\n",
      "Finished Layer 2\n",
      "\n",
      "--- Pruning Layer 3 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 3, Sample 0\n",
      "Layer 3, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 3\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 3\n",
      "Pruned self_attn.k_proj in 0.76s\n",
      "Pruning sublayer self_attn.v_proj in layer 3\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 3\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 3\n",
      "Pruned mlp.gate_proj in 0.74s\n",
      "Pruning sublayer mlp.up_proj in layer 3\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 3\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 3 with forward pass...\n",
      "Finished Layer 3\n",
      "\n",
      "--- Pruning Layer 4 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 4, Sample 0\n",
      "Layer 4, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 4\n",
      "Pruned self_attn.q_proj in 0.98s\n",
      "Pruning sublayer self_attn.k_proj in layer 4\n",
      "Pruned self_attn.k_proj in 0.72s\n",
      "Pruning sublayer self_attn.v_proj in layer 4\n",
      "Pruned self_attn.v_proj in 0.72s\n",
      "Pruning sublayer self_attn.o_proj in layer 4\n",
      "Pruned self_attn.o_proj in 0.72s\n",
      "Pruning sublayer mlp.gate_proj in layer 4\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 4\n",
      "Pruned mlp.up_proj in 0.76s\n",
      "Pruning sublayer mlp.down_proj in layer 4\n",
      "Pruned mlp.down_proj in 2.11s\n",
      "Validating pruned layer 4 with forward pass...\n",
      "Finished Layer 4\n",
      "\n",
      "--- Pruning Layer 5 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 5, Sample 0\n",
      "Layer 5, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 5\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 5\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 5\n",
      "Pruned self_attn.v_proj in 0.72s\n",
      "Pruning sublayer self_attn.o_proj in layer 5\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 5\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 5\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 5\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 5 with forward pass...\n",
      "Finished Layer 5\n",
      "\n",
      "--- Pruning Layer 6 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 6, Sample 0\n",
      "Layer 6, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 6\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 6\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 6\n",
      "Pruned self_attn.v_proj in 0.72s\n",
      "Pruning sublayer self_attn.o_proj in layer 6\n",
      "Pruned self_attn.o_proj in 0.79s\n",
      "Pruning sublayer mlp.gate_proj in layer 6\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 6\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 6\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 6 with forward pass...\n",
      "Finished Layer 6\n",
      "\n",
      "--- Pruning Layer 7 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 7, Sample 0\n",
      "Layer 7, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 7\n",
      "Pruned self_attn.q_proj in 0.98s\n",
      "Pruning sublayer self_attn.k_proj in layer 7\n",
      "Pruned self_attn.k_proj in 0.72s\n",
      "Pruning sublayer self_attn.v_proj in layer 7\n",
      "Pruned self_attn.v_proj in 0.74s\n",
      "Pruning sublayer self_attn.o_proj in layer 7\n",
      "Pruned self_attn.o_proj in 0.78s\n",
      "Pruning sublayer mlp.gate_proj in layer 7\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 7\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 7\n",
      "Pruned mlp.down_proj in 2.14s\n",
      "Validating pruned layer 7 with forward pass...\n",
      "Finished Layer 7\n",
      "\n",
      "--- Pruning Layer 8 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 8, Sample 0\n",
      "Layer 8, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 8\n",
      "Pruned self_attn.q_proj in 0.98s\n",
      "Pruning sublayer self_attn.k_proj in layer 8\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 8\n",
      "Pruned self_attn.v_proj in 0.72s\n",
      "Pruning sublayer self_attn.o_proj in layer 8\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 8\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 8\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 8\n",
      "Pruned mlp.down_proj in 2.18s\n",
      "Validating pruned layer 8 with forward pass...\n",
      "Finished Layer 8\n",
      "\n",
      "--- Pruning Layer 9 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 9, Sample 0\n",
      "Layer 9, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 9\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 9\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 9\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 9\n",
      "Pruned self_attn.o_proj in 0.72s\n",
      "Pruning sublayer mlp.gate_proj in layer 9\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 9\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 9\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 9 with forward pass...\n",
      "Finished Layer 9\n",
      "\n",
      "--- Pruning Layer 10 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 10, Sample 0\n",
      "Layer 10, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 10\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 10\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 10\n",
      "Pruned self_attn.v_proj in 0.76s\n",
      "Pruning sublayer self_attn.o_proj in layer 10\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 10\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 10\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 10\n",
      "Pruned mlp.down_proj in 2.21s\n",
      "Validating pruned layer 10 with forward pass...\n",
      "Finished Layer 10\n",
      "\n",
      "--- Pruning Layer 11 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 11, Sample 0\n",
      "Layer 11, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 11\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 11\n",
      "Pruned self_attn.k_proj in 0.72s\n",
      "Pruning sublayer self_attn.v_proj in layer 11\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 11\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 11\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 11\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 11\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 11 with forward pass...\n",
      "Finished Layer 11\n",
      "\n",
      "--- Pruning Layer 12 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 12, Sample 0\n",
      "Layer 12, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 12\n",
      "Pruned self_attn.q_proj in 1.00s\n",
      "Pruning sublayer self_attn.k_proj in layer 12\n",
      "Pruned self_attn.k_proj in 0.74s\n",
      "Pruning sublayer self_attn.v_proj in layer 12\n",
      "Pruned self_attn.v_proj in 0.76s\n",
      "Pruning sublayer self_attn.o_proj in layer 12\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 12\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 12\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 12\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 12 with forward pass...\n",
      "Finished Layer 12\n",
      "\n",
      "--- Pruning Layer 13 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 13, Sample 0\n",
      "Layer 13, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 13\n",
      "Pruned self_attn.q_proj in 1.03s\n",
      "Pruning sublayer self_attn.k_proj in layer 13\n",
      "Pruned self_attn.k_proj in 0.72s\n",
      "Pruning sublayer self_attn.v_proj in layer 13\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 13\n",
      "Pruned self_attn.o_proj in 0.72s\n",
      "Pruning sublayer mlp.gate_proj in layer 13\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 13\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 13\n",
      "Pruned mlp.down_proj in 2.12s\n",
      "Validating pruned layer 13 with forward pass...\n",
      "Finished Layer 13\n",
      "\n",
      "--- Pruning Layer 14 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 14, Sample 0\n",
      "Layer 14, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 14\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 14\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 14\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 14\n",
      "Pruned self_attn.o_proj in 0.74s\n",
      "Pruning sublayer mlp.gate_proj in layer 14\n",
      "Pruned mlp.gate_proj in 0.77s\n",
      "Pruning sublayer mlp.up_proj in layer 14\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 14\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 14 with forward pass...\n",
      "Finished Layer 14\n",
      "\n",
      "--- Pruning Layer 15 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 15, Sample 0\n",
      "Layer 15, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 15\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 15\n",
      "Pruned self_attn.k_proj in 0.72s\n",
      "Pruning sublayer self_attn.v_proj in layer 15\n",
      "Pruned self_attn.v_proj in 0.79s\n",
      "Pruning sublayer self_attn.o_proj in layer 15\n",
      "Pruned self_attn.o_proj in 0.76s\n",
      "Pruning sublayer mlp.gate_proj in layer 15\n",
      "Pruned mlp.gate_proj in 0.74s\n",
      "Pruning sublayer mlp.up_proj in layer 15\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 15\n",
      "Pruned mlp.down_proj in 2.19s\n",
      "Validating pruned layer 15 with forward pass...\n",
      "Finished Layer 15\n",
      "\n",
      "--- Pruning Layer 16 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 16, Sample 0\n",
      "Layer 16, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 16\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 16\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 16\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 16\n",
      "Pruned self_attn.o_proj in 0.74s\n",
      "Pruning sublayer mlp.gate_proj in layer 16\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 16\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 16\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 16 with forward pass...\n",
      "Finished Layer 16\n",
      "\n",
      "--- Pruning Layer 17 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 17, Sample 0\n",
      "Layer 17, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 17\n",
      "Pruned self_attn.q_proj in 0.98s\n",
      "Pruning sublayer self_attn.k_proj in layer 17\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 17\n",
      "Pruned self_attn.v_proj in 0.74s\n",
      "Pruning sublayer self_attn.o_proj in layer 17\n",
      "Pruned self_attn.o_proj in 0.75s\n",
      "Pruning sublayer mlp.gate_proj in layer 17\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 17\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 17\n",
      "Pruned mlp.down_proj in 2.14s\n",
      "Validating pruned layer 17 with forward pass...\n",
      "Finished Layer 17\n",
      "\n",
      "--- Pruning Layer 18 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 18, Sample 0\n",
      "Layer 18, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 18\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 18\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 18\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 18\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 18\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 18\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 18\n",
      "Pruned mlp.down_proj in 2.18s\n",
      "Validating pruned layer 18 with forward pass...\n",
      "Finished Layer 18\n",
      "\n",
      "--- Pruning Layer 19 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 19, Sample 0\n",
      "Layer 19, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 19\n",
      "Pruned self_attn.q_proj in 1.00s\n",
      "Pruning sublayer self_attn.k_proj in layer 19\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 19\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 19\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 19\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 19\n",
      "Pruned mlp.up_proj in 0.74s\n",
      "Pruning sublayer mlp.down_proj in layer 19\n",
      "Pruned mlp.down_proj in 2.23s\n",
      "Validating pruned layer 19 with forward pass...\n",
      "Finished Layer 19\n",
      "\n",
      "--- Pruning Layer 20 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 20, Sample 0\n",
      "Layer 20, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 20\n",
      "Pruned self_attn.q_proj in 1.00s\n",
      "Pruning sublayer self_attn.k_proj in layer 20\n",
      "Pruned self_attn.k_proj in 0.77s\n",
      "Pruning sublayer self_attn.v_proj in layer 20\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 20\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 20\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 20\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 20\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 20 with forward pass...\n",
      "Finished Layer 20\n",
      "\n",
      "--- Pruning Layer 21 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 21, Sample 0\n",
      "Layer 21, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 21\n",
      "Pruned self_attn.q_proj in 0.98s\n",
      "Pruning sublayer self_attn.k_proj in layer 21\n",
      "Pruned self_attn.k_proj in 0.72s\n",
      "Pruning sublayer self_attn.v_proj in layer 21\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 21\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 21\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 21\n",
      "Pruned mlp.up_proj in 0.76s\n",
      "Pruning sublayer mlp.down_proj in layer 21\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 21 with forward pass...\n",
      "Finished Layer 21\n",
      "\n",
      "--- Pruning Layer 22 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 22, Sample 0\n",
      "Layer 22, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 22\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 22\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 22\n",
      "Pruned self_attn.v_proj in 0.73s\n",
      "Pruning sublayer self_attn.o_proj in layer 22\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 22\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 22\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 22\n",
      "Pruned mlp.down_proj in 2.14s\n",
      "Validating pruned layer 22 with forward pass...\n",
      "Finished Layer 22\n",
      "\n",
      "--- Pruning Layer 23 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 23, Sample 0\n",
      "Layer 23, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 23\n",
      "Pruned self_attn.q_proj in 1.00s\n",
      "Pruning sublayer self_attn.k_proj in layer 23\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 23\n",
      "Pruned self_attn.v_proj in 0.72s\n",
      "Pruning sublayer self_attn.o_proj in layer 23\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 23\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 23\n",
      "Pruned mlp.up_proj in 0.74s\n",
      "Pruning sublayer mlp.down_proj in layer 23\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 23 with forward pass...\n",
      "Finished Layer 23\n",
      "\n",
      "--- Pruning Layer 24 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 24, Sample 0\n",
      "Layer 24, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 24\n",
      "Pruned self_attn.q_proj in 0.98s\n",
      "Pruning sublayer self_attn.k_proj in layer 24\n",
      "Pruned self_attn.k_proj in 0.75s\n",
      "Pruning sublayer self_attn.v_proj in layer 24\n",
      "Pruned self_attn.v_proj in 0.81s\n",
      "Pruning sublayer self_attn.o_proj in layer 24\n",
      "Pruned self_attn.o_proj in 0.79s\n",
      "Pruning sublayer mlp.gate_proj in layer 24\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 24\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 24\n",
      "Pruned mlp.down_proj in 2.15s\n",
      "Validating pruned layer 24 with forward pass...\n",
      "Finished Layer 24\n",
      "\n",
      "--- Pruning Layer 25 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 25, Sample 0\n",
      "Layer 25, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 25\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 25\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 25\n",
      "Pruned self_attn.v_proj in 0.72s\n",
      "Pruning sublayer self_attn.o_proj in layer 25\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 25\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 25\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 25\n",
      "Pruned mlp.down_proj in 2.16s\n",
      "Validating pruned layer 25 with forward pass...\n",
      "Finished Layer 25\n",
      "\n",
      "--- Pruning Layer 26 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 26, Sample 0\n",
      "Layer 26, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 26\n",
      "Pruned self_attn.q_proj in 0.99s\n",
      "Pruning sublayer self_attn.k_proj in layer 26\n",
      "Pruned self_attn.k_proj in 0.73s\n",
      "Pruning sublayer self_attn.v_proj in layer 26\n",
      "Pruned self_attn.v_proj in 0.72s\n",
      "Pruning sublayer self_attn.o_proj in layer 26\n",
      "Pruned self_attn.o_proj in 0.72s\n",
      "Pruning sublayer mlp.gate_proj in layer 26\n",
      "Pruned mlp.gate_proj in 0.74s\n",
      "Pruning sublayer mlp.up_proj in layer 26\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 26\n",
      "Pruned mlp.down_proj in 2.14s\n",
      "Validating pruned layer 26 with forward pass...\n",
      "Finished Layer 26\n",
      "\n",
      "--- Pruning Layer 27 ---\n",
      "Finding prunable sublayers...\n",
      "Found 7 sublayers to prune.\n",
      "Initialized SparseGPT for self_attn.q_proj\n",
      "Initialized SparseGPT for self_attn.k_proj\n",
      "Initialized SparseGPT for self_attn.v_proj\n",
      "Initialized SparseGPT for self_attn.o_proj\n",
      "Initialized SparseGPT for mlp.gate_proj\n",
      "Initialized SparseGPT for mlp.up_proj\n",
      "Initialized SparseGPT for mlp.down_proj\n",
      "Registering forward hook for self_attn.q_proj\n",
      "Registering forward hook for self_attn.k_proj\n",
      "Registering forward hook for self_attn.v_proj\n",
      "Registering forward hook for self_attn.o_proj\n",
      "Registering forward hook for mlp.gate_proj\n",
      "Registering forward hook for mlp.up_proj\n",
      "Registering forward hook for mlp.down_proj\n",
      "Running forward passes for calibration...\n",
      "Layer 27, Sample 0\n",
      "Layer 27, Sample 1\n",
      "Removing forward hooks.\n",
      "Pruning sublayer self_attn.q_proj in layer 27\n",
      "Pruned self_attn.q_proj in 1.00s\n",
      "Pruning sublayer self_attn.k_proj in layer 27\n",
      "Pruned self_attn.k_proj in 0.74s\n",
      "Pruning sublayer self_attn.v_proj in layer 27\n",
      "Pruned self_attn.v_proj in 0.76s\n",
      "Pruning sublayer self_attn.o_proj in layer 27\n",
      "Pruned self_attn.o_proj in 0.73s\n",
      "Pruning sublayer mlp.gate_proj in layer 27\n",
      "Pruned mlp.gate_proj in 0.73s\n",
      "Pruning sublayer mlp.up_proj in layer 27\n",
      "Pruned mlp.up_proj in 0.73s\n",
      "Pruning sublayer mlp.down_proj in layer 27\n",
      "Pruned mlp.down_proj in 2.13s\n",
      "Validating pruned layer 27 with forward pass...\n",
      "Finished Layer 27\n",
      "All layers pruned.\n",
      "Pruning complete --- Saving pruned model...\n",
      "Validating parameter count after full pruning and reconfiguration...\n",
      "✅ Final Pruned Model Parameters: 3212.75M\n",
      "Compression ratio: 0.00%\n",
      "Saving pruned model to pruned-llama-3b_instruct_sparsegpt...\n",
      "Pruned model saved successfully to pruned-llama-3b_instruct_sparsegpt\n",
      "******************************\n",
      "layer 0 sparsity 0.200002\n",
      "layer 1 sparsity 0.200002\n",
      "layer 2 sparsity 0.200002\n",
      "layer 3 sparsity 0.200002\n",
      "layer 4 sparsity 0.200002\n",
      "layer 5 sparsity 0.200002\n",
      "layer 6 sparsity 0.200002\n",
      "layer 7 sparsity 0.200002\n",
      "layer 8 sparsity 0.200002\n",
      "layer 9 sparsity 0.200002\n",
      "layer 10 sparsity 0.200002\n",
      "layer 11 sparsity 0.200002\n",
      "layer 12 sparsity 0.200002\n",
      "layer 13 sparsity 0.200002\n",
      "layer 14 sparsity 0.200002\n",
      "layer 15 sparsity 0.200002\n",
      "layer 16 sparsity 0.200002\n",
      "layer 17 sparsity 0.200002\n",
      "layer 18 sparsity 0.200002\n",
      "layer 19 sparsity 0.200002\n",
      "layer 20 sparsity 0.200002\n",
      "layer 21 sparsity 0.200002\n",
      "layer 22 sparsity 0.200002\n",
      "layer 23 sparsity 0.200002\n",
      "layer 24 sparsity 0.200002\n",
      "layer 25 sparsity 0.200002\n",
      "layer 26 sparsity 0.200002\n",
      "layer 27 sparsity 0.200002\n",
      "sparsity sanity check 0.2000\n",
      "******************************\n",
      "Final parameters: 3212.75M\n",
      "Compression ratio: 0.00%\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function for measuing the number of weights / parameters zeroed out of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:43:23.191998Z",
     "iopub.status.busy": "2025-07-26T18:43:23.191720Z",
     "iopub.status.idle": "2025-07-26T18:43:23.197621Z",
     "shell.execute_reply": "2025-07-26T18:43:23.196941Z",
     "shell.execute_reply.started": "2025-07-26T18:43:23.191981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def load_model_stats(model_path):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True, torch_dtype=torch.float16)  # or float32\n",
    "    model.to(\"cpu\")  # or \"cuda\" if your GPU supports it\n",
    "\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    nonzero_params = sum((p != 0).sum().item() for p in model.parameters() if p.requires_grad) /1e6\n",
    "\n",
    "    # size_gb = sum(os.path.getsize(os.path.join(dp, f)) for dp, _, fs in os.walk(model_path) for f in fs) / 1e9\n",
    "\n",
    "    print(f\"Total Parameters      : {total_params:.2f} million\")\n",
    "    print(f\"Non-zero Parameters   : {nonzero_params:.2f} million\")\n",
    "    print(f\"Sparsity              : {(1 - nonzero_params / total_params) * 100:.2f}%\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:43:23.198926Z",
     "iopub.status.busy": "2025-07-26T18:43:23.198612Z",
     "iopub.status.idle": "2025-07-26T18:43:57.368664Z",
     "shell.execute_reply": "2025-07-26T18:43:57.367827Z",
     "shell.execute_reply.started": "2025-07-26T18:43:23.198903Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b3dc70ffb0407fbae9c22e60a31dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters      : 3212.75 million\n",
      "Non-zero Parameters   : 2649.34 million\n",
      "Sparsity              : 17.54%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_stats(\"/kaggle/working/DSnoT/pruned_llama3b_instruct_dsnot_with_sparsegpt\") ## give the path where the pruned model is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-26T17:48:28.911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### ALL METHODS OF PRUNING DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:45:31.705433Z",
     "iopub.status.busy": "2025-07-26T18:45:31.704756Z",
     "iopub.status.idle": "2025-07-26T18:45:32.706079Z",
     "shell.execute_reply": "2025-07-26T18:45:32.705479Z",
     "shell.execute_reply.started": "2025-07-26T18:45:31.705408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete all relevant variables\n",
    "# del model  # Replace with your model variable name\n",
    "# del inputs, outputs  # Replace with any other tensors or variables\n",
    "gc.collect()  # Run Python garbage collector\n",
    "torch.cuda.empty_cache()  # Clear PyTorch's GPU cacheimport torch\n",
    "\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch's CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Optional: Also clear Python's garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:45:34.123185Z",
     "iopub.status.busy": "2025-07-26T18:45:34.122860Z",
     "iopub.status.idle": "2025-07-26T18:45:34.131209Z",
     "shell.execute_reply": "2025-07-26T18:45:34.130358Z",
     "shell.execute_reply.started": "2025-07-26T18:45:34.123162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory of GPU: 8.12 MB\n",
      "Cached Memory of GPU: 20.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory of GPU: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory of GPU: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find the folder size of the pruned model to see the size diff b/w original and pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:45:38.932795Z",
     "iopub.status.busy": "2025-07-26T18:45:38.932002Z",
     "iopub.status.idle": "2025-07-26T18:45:38.937709Z",
     "shell.execute_reply": "2025-07-26T18:45:38.936857Z",
     "shell.execute_reply.started": "2025-07-26T18:45:38.932770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                total_size += os.path.getsize(file_path)\n",
    "            except OSError as e:\n",
    "                print(f\"Error getting size of {file_path}: {e}\")\n",
    "    return total_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T17:35:47.618381Z",
     "iopub.status.busy": "2025-07-26T17:35:47.617893Z",
     "iopub.status.idle": "2025-07-26T17:35:47.624477Z",
     "shell.execute_reply": "2025-07-26T17:35:47.623697Z",
     "shell.execute_reply.started": "2025-07-26T17:35:47.618338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE OF ORIGINAL MODEL IS : \n",
      "Folder size: 12,851,102,108 bytes\n",
      "Folder size: 12549904.40 KB\n",
      "Folder size: 12255.77 MB\n"
     ]
    }
   ],
   "source": [
    "# Path of your Original model to see if it is pruned and now smaller than original \n",
    "folder_path = \"/kaggle/working/DSnoT/llama-3b-weights\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    size_in_bytes = get_folder_size(folder_path)\n",
    "    size_in_kb = size_in_bytes / 1024\n",
    "    size_in_mb = size_in_kb / 1024\n",
    "    print(\"SIZE OF ORIGINAL MODEL IS : \")\n",
    "    print(f\"Folder size: {size_in_bytes:,} bytes\")\n",
    "    print(f\"Folder size: {size_in_kb:.2f} KB\")\n",
    "    print(f\"Folder size: {size_in_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"Folder not found: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:45:43.502914Z",
     "iopub.status.busy": "2025-07-26T18:45:43.502632Z",
     "iopub.status.idle": "2025-07-26T18:45:43.508596Z",
     "shell.execute_reply": "2025-07-26T18:45:43.507993Z",
     "shell.execute_reply.started": "2025-07-26T18:45:43.502894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE OF PRUNED MODEL IS : \n",
      "Folder size: 6,442,815,753 bytes\n",
      "Folder size: 6291812.26 KB\n",
      "Folder size: 6144.35 MB\n"
     ]
    }
   ],
   "source": [
    "# Path of your pruned model to see if it is pruned and now smaller than original \n",
    "folder_path = \"/kaggle/working/DSnoT/pruned_llama3b_instruct_dsnot_with_sparsegpt\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    size_in_bytes = get_folder_size(folder_path)\n",
    "    size_in_kb = size_in_bytes / 1024\n",
    "    size_in_mb = size_in_kb / 1024\n",
    "    print(\"SIZE OF PRUNED MODEL IS : \")\n",
    "    print(f\"Folder size: {size_in_bytes:,} bytes\")\n",
    "    print(f\"Folder size: {size_in_kb:.2f} KB\")\n",
    "    print(f\"Folder size: {size_in_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"Folder not found: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T06:57:00.034587Z",
     "iopub.status.busy": "2025-07-04T06:57:00.034264Z",
     "iopub.status.idle": "2025-07-04T06:57:00.040250Z",
     "shell.execute_reply": "2025-07-04T06:57:00.039483Z",
     "shell.execute_reply.started": "2025-07-04T06:57:00.034565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Allocated Memory of GPU: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory of GPU: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPLOAD THE MODEL TO HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T02:50:01.639834Z",
     "iopub.status.busy": "2025-07-30T02:50:01.638523Z",
     "iopub.status.idle": "2025-07-30T02:50:50.646480Z",
     "shell.execute_reply": "2025-07-30T02:50:50.645817Z",
     "shell.execute_reply.started": "2025-07-30T02:50:01.639813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84c50205494415b9d4a388acb389a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/5.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AlphaAnas70/pruned-llama-3b_torch_prune-l1_20_per/commit/9bafe4d91c07eaff5059051a2cda963d10334145', commit_message='Pushing sparsegpt pruned LLaMA model', commit_description='', oid='9bafe4d91c07eaff5059051a2cda963d10334145', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AlphaAnas70/pruned-llama-3b_torch_prune-l1_20_per', endpoint='https://huggingface.co', repo_type='model', repo_id='AlphaAnas70/pruned-llama-3b_torch_prune-l1_20_per'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create repo\n",
    "repo_id = \"AlphaAnas70/pruned-llama-3b_torch_prune-l1_20_per\"\n",
    "create_repo(repo_id, exist_ok=True)\n",
    "\n",
    "# Upload model folder\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=\"/kaggle/working/DSnoT/pruned-llama-3b_torch_prune-l1_20_per\",\n",
    "    commit_message=\"Pushing sparsegpt pruned LLaMA model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to see if the model is now working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:52:56.942690Z",
     "iopub.status.busy": "2025-07-31T02:52:56.942437Z",
     "iopub.status.idle": "2025-07-31T02:52:57.063118Z",
     "shell.execute_reply": "2025-07-31T02:52:57.061720Z",
     "shell.execute_reply.started": "2025-07-31T02:52:56.942667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'check_fp8_capability' from 'accelerate.utils.environment' (/usr/local/lib/python3.11/dist-packages/accelerate/utils/environment.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"__init__.py\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madjacent_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m         \u001b[0madjacent_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__init__.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlignDevicesHook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_hook_to_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m from .big_modeling import (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcheckpointing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_custom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_custom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoaderDispatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_first_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/checkpointing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 62\u001b[0;31m from .environment import (\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mare_libraries_initialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'check_fp8_capability' from 'accelerate.utils.environment' (/usr/local/lib/python3.11/dist-packages/accelerate/utils/environment.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"__init__.py\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madjacent_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m         \u001b[0madjacent_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__init__.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .auto_factory import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0m_BaseAutoBackboneClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m                 \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m     \u001b[0;31m# Modular files should not be imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_substring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'check_fp8_capability' from 'accelerate.utils.environment' (/usr/local/lib/python3.11/dist-packages/accelerate/utils/environment.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/1098432129.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"AlphaAnas70/pruned_llama-3.2-3b-Instruct-20_per\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m                 \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m                 \u001b[0madjacent_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"__pycache__\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m                 \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m                 \u001b[0madjacent_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0madjacent_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__init__.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m     \u001b[0;31m# Modular files should not be imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_substring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'check_fp8_capability' from 'accelerate.utils.environment' (/usr/local/lib/python3.11/dist-packages/accelerate/utils/environment.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "repo_id = \"AlphaAnas70/pruned_llama-3.2-3b-Instruct-20_per\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id,  use_fast=False)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-31T02:52:57.063680Z",
     "iopub.status.idle": "2025-07-31T02:52:57.063977Z",
     "shell.execute_reply": "2025-07-31T02:52:57.063836Z",
     "shell.execute_reply.started": "2025-07-31T02:52:57.063824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# Decode and print\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOMLY CHECKING JUST A MODEL THAT IS PRUNED ALR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T16:33:16.850306Z",
     "iopub.status.busy": "2025-08-12T16:33:16.849638Z",
     "iopub.status.idle": "2025-08-12T16:34:42.768321Z",
     "shell.execute_reply": "2025-08-12T16:34:42.767482Z",
     "shell.execute_reply.started": "2025-08-12T16:33:16.850275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b3bbf54f3b4a6eb56afb5d022d8828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:33:31.629847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755016411.970168      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755016412.066731      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2f684b4561483988c187614498ce54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0c3643315d41e0a41ea5e3d538ed6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bfe07a741e48379f64716f9de131c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5307aa16d34479c861fca85d9e6ac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b82f9a4504c4a3d86b90e82fa6b9159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec0948f59694c5a979ee1fbad8ef630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 3212.75M\n",
      "Non-zero params: 2649.03M\n",
      "Sparsity: 17.55%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AlphaAnas70/pruned_llama3b_sparsegpt_17.55_per\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "nonzero_params = sum((p != 0).sum().item() for p in model.parameters()) / 1e6\n",
    "sparsity = (1 - nonzero_params / total_params) * 100\n",
    "\n",
    "print(f\"Total params: {total_params:.2f}M\")\n",
    "print(f\"Non-zero params: {nonzero_params:.2f}M\")\n",
    "print(f\"Sparsity: {sparsity:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
